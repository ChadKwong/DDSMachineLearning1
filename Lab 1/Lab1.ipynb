{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f98d1ef5",
   "metadata": {},
   "source": [
    "# Machine Learning 1 Lab 1\n",
    "\n",
    "## Alex Lopez, Chad Kwong, Chris Haub, Erin McClure-Price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9288a2c5",
   "metadata": {},
   "source": [
    "### Loading In Data and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d374e30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'bank-additional-full.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-362346bb45e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#Import Dataset from github repo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mbankData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bank-additional-full.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unknown\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m#Optional: Add in index column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#bankData.insert(0, 'Sample_ID', range(1,len(bankData)+1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bank-additional-full.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "#Suppress futurewarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#Set Figure Size\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "#Import Dataset from github repo\n",
    "bankData = pd.read_csv('bank-additional-full.csv', sep=';', na_values=\"unknown\")\n",
    "#Optional: Add in index column\n",
    "#bankData.insert(0, 'Sample_ID', range(1,len(bankData)+1))\n",
    "\n",
    "#Creating variables for indexing continuous and categorical variables\n",
    "conCol = ['age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate', \n",
    "          'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "catCol = ['job', 'marital', 'education', 'default', 'housing', 'loan', \n",
    "          'contact', 'month', 'day_of_week', 'poutcome', 'y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef32919",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdff1b3",
   "metadata": {},
   "source": [
    "#### Describe the purpose of the data set you selected (i.e., why was this data collected in the first place?). Describe how you would define and measure the outcomes from the data set. That is, why is this data important and how do you know if you have mined useful knowledge from the data set? How would you measure the effectiveness of a good prediction algorithm? Be specific."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43272bb4",
   "metadata": {},
   "source": [
    "For this project, we chose to use the \"Bank Marketing Data Set\" that is stored in the Machine Learning Repository by the University of California, Irvine. The initial purpose of the data was to predict whether a customer would subscribe to a long-term deposit, which would allow the institution to better pursue those customers specifically. The data set contains 21 attributes and 41,188 instances collected via a telephone marketing campaign led by a “Portuguese banking institution” (Moro et al June 2014). While the data set only contains one prescribed target outcome (Long Term Deposits), we chose to additionally perform a second predicition. The second prediction we decided to create was what level of education (i.e., high school, years of college, illiteracy, etc.) is associated with increased potential of taking a long-term deposit. (S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier,\n",
    "62:22-31, June 2014)\n",
    "\n",
    "To predict our two desired outcomes, we constructed models using the remaining predictors. We hypothesized that success from our models would increase the efficiency of our telemarketing team regarding long-term deposits. In addition, the type of customer who is likely to take a long-term deposit can be readily distinguished and targeted, thereby increasing the number of long-term deposits held by the institution.\n",
    "\n",
    "Predicting **education** and **term deposit** resulted in us solving a classification problem. Ultimately we decided in predicting **term deposit** through Logistic Regression to provide some room for interpretation, and we chose a more abstract clustering model to predict the **education** of the person of interest. The effectiveness of the classification model will be measured through metrics such as accuracy and recall, where recall is equal to the number of true positives divided by the sum of true positives and false negatives (sensitivity). Additionally, to ensure the strength of our model, we will perform 10 fold cross validation as an attempt to construct a model that will perform well with other data sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd63a6ee",
   "metadata": {},
   "source": [
    "**Sources:** \n",
    "\n",
    "S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the\n",
    "Success of Bank Telemarketing. Decision Support Systems, Elsevier,\n",
    "62:22-31, June 2014\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/bank+marketing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1813ee",
   "metadata": {},
   "source": [
    "## Data Meaning Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e63739",
   "metadata": {},
   "source": [
    "#### Describe the meaning and type of data (scale, values, etc.) for each attribute in the data file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a8474c",
   "metadata": {},
   "source": [
    "As stated above, the data set contains 21 attributes and 41,188 instances. A complete breakdown of the attributes with their data type and their data description was obtained from the following address: https://archive.ics.uci.edu/ml/datasets/bank+marketing. A table was constructed from this information and was presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d47e43b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import image module\n",
    "from IPython.display import Image\n",
    "  \n",
    "# get the image\n",
    "Image(url=\"Screenshot.png\", width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb03e79",
   "metadata": {},
   "source": [
    "## Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae73dfd1",
   "metadata": {},
   "source": [
    "#### \tVerify data quality: Explain any missing values, duplicate data, and outliers. Are those mistakes? How do you deal with these problems? Give justifications for your methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6e1d59",
   "metadata": {},
   "source": [
    "The full data set contained 41,188 rows. Of these, 10,700 contain NaN values. Due to the size of the data set, we decided to omit these missing values. We acknowledge that the omitted data may be representative of a separate population, but given that we still have greater than 30 thousand instances without them, we decided to remove the data for now with the option of using them for additional investigation at a later date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213fd68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bankData = bankData.dropna()\n",
    "bankData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1df02f",
   "metadata": {},
   "source": [
    "After an initial investigation, we found that the dataset was imbalanced with extreme imbalance presented in the **default** variable. This imbalance brought concerns to constructing an accurate model for external data sets when predicting this particular variable. While we could have used N-fold cross validation to overcome this obstruction, we chose to simply choose a different variable as our second response variable to predict. The code below demonstrates the imbalance in the **default** variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dffdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Value Counts for Default Attribute:\\n\\n\",bankData.default.value_counts(),\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445e774a",
   "metadata": {},
   "source": [
    "## Simple Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ef13d2",
   "metadata": {},
   "source": [
    "#### Visualize appropriate statistics (e.g., range, mode, mean, median, variance, counts) for a subset of attributes. Describe anything meaningful you found from this or if you found something potentially interesting. Note: You can also use data from other sources for comparison. Explain why the statistics run are meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027224f0",
   "metadata": {},
   "source": [
    "The following code returns basic statistics for the continous variables. We see that for most of the variables the mean value is relatively close to the median, meaning that their distributions are close to normal. \n",
    "\n",
    "The distribution was highly skewed for the **duration** attribute, which lists the amount of time the customer spent on the call. However, as noted in the \"Data Meaning Type\" section above, this variable should be removed because the time spent on the call cannot be known until after the call is ended. Obviously, those callers who take out a long-**term deposit** will naturally spend more time on the call, which indicates that this variable is likely to skew results in a model.\n",
    "\n",
    "We also observed heavy skewness in the **pdays** and **previous** variables denoted by the equality between the 1st, 2nd and 3rd quartiles in the variables. While simple statistics present heavy skewness that may suggest omittion from many models, in the future we will focus the modeling methods around those that are not limited to a parameteric distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb8c4d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Getting basic statistical overview of continuous variables using the describe function\n",
    "bankData.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c121c4",
   "metadata": {},
   "source": [
    "The following code gathers the counts of each categorical column and displays the percentage of each response within the column. **Job** percentages were skewed with administration jobs having 29% of the data, followed by more blue collar professions, specifically blue collar and technicians. Percentages for **marital status** was 57% married, 31% single and 12% divorced. The largest **education** group was subjects with a bachelor degree at 34% followed by people with high school diploma at 25%. The smallest contacted group was people who were illiterate at less than 1%. The percentages of subjects that have **default** was less than 1%. This makes sense as people most likely to have money to save would not be defaulting, so would be most likley to contact that group. Most likely people that do not have defaults were targeted to be contacted. If they had a **housing** loan was 54% verses 46% that did not, so did not play a large factor. 84% of those contacted did not have a personal **loan** while 16% did. 67% of subjects were **contact** by cell phone while 33% were **contact** by traditional land line. The heavist **month** of contact being made was May at 32% respectively followed by July and August at 17% and 15%. The summer months had most **contact** overall. The least **contact** month was December at less than 1%. For **day of week** all contact was made Monday through Friday with most days being close to equal at roughly 20% per day. Friday was the least contacted day at 19%. The outcomes from **previous** campaigns was 85% not part of a **previous** campaign, 11% failed to open a deposit account and 4% opened a deposit account. 13% of respondents **subscribed a term depost** when contacted while 87% did not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212a998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCP(column):\n",
    "    xCount = column.value_counts()\n",
    "    xPercentages = xCount/len(column)\n",
    "    outData = pd.DataFrame()\n",
    "    outData['Counts'] = xCount\n",
    "    outData['Percent of Total'] = xPercentages\n",
    "    return outData\n",
    "\n",
    "for i in catCol:\n",
    "    print(\"Percentages for \",i,\" : \\n\",getCP(bankData[i]),\"\\n\\n\")\n",
    "    \n",
    "getCP(pdays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26146207",
   "metadata": {},
   "source": [
    "## Visualize Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138f02c1",
   "metadata": {},
   "source": [
    "#### Visualize the most interesting attributes (at least 5 attributes, your opinion on what is interesting). Important: Interpret the implications for each visualization. Explain for each attribute why the chosen visualization is appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2f2cb2",
   "metadata": {},
   "source": [
    "The following code defines a functions that produce histograms and boxplots for continuous variables and count plots with percentages for categorical variables. This code was obtained from the University of Texas Post Graduate Program in Artifical Intelligence and Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e9fe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create plots for continous variables\n",
    "def histobox(feature, figsize=(15, 10), bins=None):\n",
    "    \"\"\"Boxplot and histogram combined\n",
    "    feature: 1-d feature array\n",
    "    figsize: size of fig (default (9,8))\n",
    "    bins: number of bins (default None / auto)\n",
    "    \"\"\"\n",
    "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
    "        nrows=2,  # Number of rows of the subplot grid= 2\n",
    "        sharex=True,  # x-axis will be shared among all subplots\n",
    "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
    "        figsize=figsize,\n",
    "    )  # creating the 2 subplots\n",
    "    sns.boxplot(\n",
    "        feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
    "    )  # boxplot will be created and a star will indicate the mean value of the column\n",
    "    sns.distplot(\n",
    "        feature, kde=F, ax=ax_hist2, bins=bins, color=\"orange\"\n",
    "    ) if bins else sns.distplot(\n",
    "        feature, kde=False, ax=ax_hist2, color=\"tab:cyan\"\n",
    "    )  # For histogram\n",
    "    ax_hist2.axvline(\n",
    "        np.mean(feature), color=\"purple\", linestyle=\"--\"\n",
    "    )  # Add mean to the histogram\n",
    "    ax_hist2.axvline(\n",
    "        np.median(feature), color=\"black\", linestyle=\"-\"\n",
    "    )  # Add median to the histogram\n",
    "    \n",
    "\n",
    "# function to create plots for categorical varibales\n",
    "def perc_on_bar(plot, feature):\n",
    "    '''\n",
    "    plot\n",
    "    feature: categorical feature\n",
    "    the function won't work if a column is passed in hue parameter\n",
    "    '''\n",
    "\n",
    "    total = len(feature) # length of the column\n",
    "    for p in ax.patches:\n",
    "        percentage = '{:.1f}%'.format(100 * p.get_height()/total) # percentage of each class of the category\n",
    "        x = p.get_x() + p.get_width() / 2 - 0.05 # width of the plot\n",
    "        y = p.get_y() + p.get_height()           # hieght of the plot\n",
    "        ax.annotate(percentage, (x, y), size = 12) # annotate the percantage \n",
    "        \n",
    "    plt.show() # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb34a0c",
   "metadata": {},
   "source": [
    "### Continous Variable Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cea2a2",
   "metadata": {},
   "source": [
    "Below is a for loop that cycles through all the continuous variables and produces the boxplots and histograms by utilizing the function created above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1134630f",
   "metadata": {},
   "source": [
    "We chose to investigate the distribution of age betweens subjects in the dataset. We found evidence to suggest that target audience for these long term deposits are centered around the sample mean of 39 years old. Furthermore we observe that outliers outside 93.4% of the population occur after the age 69. This makes practical sense as the average retirement age in the United States is around 66 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee87de",
   "metadata": {},
   "outputs": [],
   "source": [
    "histobox(bankData.age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b5dad",
   "metadata": {},
   "source": [
    "We then investigated the **pdays** variable. As a reminder, this variable tracks the number of days that have lapsed since the subject was last contacted, and a value of 999 denotes that the subject has not been contacted before. From the basic statistics, we saw indication of heavy skewness. We then visually plotted this variable to get a better understanding. In order to make the data more interpretable, we performed a log transformation on the Y-Axis and adjusted the X-Axis. From the resulting plot, there is overwhelming evidence to suggest that a heavy majority of the sample population are new prospective applicants for long term deposits, but if subjects are repeatedly contacted, then the longest time between contacts is 27 days.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4df7050",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(bankData.pdays).set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8490a1ff",
   "metadata": {},
   "source": [
    "The following cell contains commented code for producing histograms and boxplots for all continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56b0b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in conCol:\n",
    "#     histobox(bankData[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5819e3c",
   "metadata": {},
   "source": [
    "### Categorical Variable Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d59fec",
   "metadata": {},
   "source": [
    "Below are count plots that were created using the above function \"perc_on_bar\". The function displays the different levels of the called categorical variable and plots the count as a bar and percentage that corresponds to the level on top of the bar. The visual provides the same information that a value count table would but offers a more dramatic perspective on the differences and possible skewness of the categorical variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09057f6b",
   "metadata": {},
   "source": [
    "Categorical data provided us with a bit more insight. To begin, we examined the distribution of the **Job**. Admin, Blue-Collar, and Technician jobs contains 64.2% of respondants. This would suggest that the bank targeted people who have, what are assumed to be, higher paying ans stable jobs. Unemployed people and students both had the lowest percentage of targeted marketing campaigns, with only 2%. There is visual evidence to suggest that ANOVA can be performed to distiguish differences between levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79867275",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = sns.countplot(bankData['job'],palette='winter')\n",
    "perc_on_bara(ax,bankData['job'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71099bb",
   "metadata": {},
   "source": [
    "Like **job**, the **marital** variable also provided somewhat interesting insight on the targeted subjects. Over 57% of the subjects were identified to be married. Further review in the bivariate analysis can help identify possible correlations between a married status and other categorical vairbles like **education** or **job**. However, intuition would suggest that a majority of the working class is likely also married with a family. Again, this will need to be furthered reviewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9393633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(bankData['marital'],palette='winter')\n",
    "perc_on_bar(ax,bankData['marital'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b4093",
   "metadata": {},
   "source": [
    "Finally, we focused our initial review on the **education** variable using the same \"prec_on_bar\" plot function. Even without ordering the x-axis of the plot, it is obvious that there is a skewness to the data. Those with more education were targeted more often with a campaign compared to those subjects with less education. More specifically, subjects with at least a high school education accounted for 59.5% of the sample popluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcfa8b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = sns.countplot(bankData['education'],palette='winter')\n",
    "perc_on_bar(ax,bankData['education'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7882e7",
   "metadata": {},
   "source": [
    "The following cell contains commented code for producing count plots with percetages above for all categorical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a68d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i in catCol:\n",
    "#     ax = sns.countplot(bankData[i],palette='winter')\n",
    "#     perc_on_bar(ax,bankData[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7836b25",
   "metadata": {},
   "source": [
    "## Explore Joint Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fbaa5f",
   "metadata": {},
   "source": [
    "#### Visualize relationships between attributes: Look at the attributes via scatter plots, correlation, cross-tabulation, group-wise averages, etc. as appropriate. Explain any interesting relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7db72ea",
   "metadata": {},
   "source": [
    "The following is a heatmap of the continuous variables. From this map, we saw strong indications of correlations between the **euribor3m** variable and the **previous**, **emp.var.rate**, and **nr.employed** variables. There was also evidence of higher correlations between the **previous** variable and the **pdays** and **emp.var.rate** variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45adf04f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = bankData\n",
    "def heatMap(data, figsize=(20,15)):\n",
    "    #establish figure size\n",
    "    f = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    #heatmap conversion\n",
    "    plt.matshow(data.corr(), fignum=f.number)\n",
    "    \n",
    "    #Change axis bars and labels\n",
    "    plt.xticks(range(data.select_dtypes(['number']).shape[1]), data.select_dtypes(['number']).columns, fontsize=14, rotation=45)\n",
    "    plt.yticks(range(data.select_dtypes(['number']).shape[1]), data.select_dtypes(['number']).columns, fontsize=14)\n",
    "    cb = plt.colorbar()\n",
    "    cb.ax.tick_params(labelsize=14)\n",
    "    plt.title('Correlation Matrix', fontsize=16);\n",
    "\n",
    "heatMap(bankData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b31de9a",
   "metadata": {},
   "source": [
    "After plotting each continuous variable in a pair plot grid, as seen below, we could not find enough evidence to suggest colinearity between the continuous variables. The only variable that would justify further investigation would be **duration**, but we have already determined that this variable should be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc715ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(bankData[conCol])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479046f0",
   "metadata": {},
   "source": [
    "We then decided to investigate the categorical variables as well through hot encoding the variables to distinguish individual relationships and correlations between levels. From the resulting plot below, we found no indications of concern between any of the levels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d98c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding of ALL categorical variables\n",
    "# pd.concat([*]], axis=1) // this line of code concatenates all the data frames in the [*] list\n",
    "# [** for col in categ_features] // this steps through each feature in categ_features and \n",
    "#                                //   creates a new element in a list based on the output of **\n",
    "# pd.get_dummies(df_imputed[col],prefix=col) // this creates a one hot encoded dataframe of the variable=col (like code above)\n",
    "\n",
    "categ_features = ['job','marital','education','default','housing','loan','contact','month','poutcome'];\n",
    "\n",
    "OneHotDF = pd.concat([pd.get_dummies(bankData[col],prefix=col,drop_first=True) for col in categ_features], axis=1)\n",
    "\n",
    "OneHotDF.head()\n",
    "\n",
    "#Code obtained from: https://github.com/jakemdrew/DataMiningNotebooks/blob/master/01.%20Pandas.ipynb\n",
    "\n",
    "#Merged OneHotDF and bankData\n",
    "\n",
    "mergedDF = pd.concat([bankData.select_dtypes(exclude='object'),OneHotDF],axis=1)\n",
    "\n",
    "heatMap(mergedDF, figsize=(28,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bad4a9",
   "metadata": {},
   "source": [
    "As referenced in the univariate analysis of the categorical variables, there was evidenece to suggest some amount of correlation between **job**, **marital**, and **education**. The count plots below are bivariate with combinations of the mentioned categorical variables. The **marital** (x-axis) status by **job** (color) plot showed correlation between married individuals and jobs that were assumed to be higher paying. **Education** by **job** revealed that highschool and college grads had the highest amount of subjects with admin job titles, providing evidenece that the higher paying jobs were held by indivduals with higher education levels. Finally, **education** by **marital** showed that no matter what the education level, most subjects were indeed married."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649d4a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "fig, (MJ, EJ, EM) = plt.subplots(nrows=3)\n",
    "\n",
    "sns.countplot(\n",
    "    data=bankData, x=\"marital\", hue=\"job\",\n",
    "    palette=\"pastel\", edgecolor=\".6\",\n",
    "    ax=MJ\n",
    ")\n",
    "sns.countplot(\n",
    "    data=bankData, x=\"education\", hue=\"job\",\n",
    "    palette=\"pastel\", edgecolor=\".6\",\n",
    "    ax=EJ\n",
    ")\n",
    "sns.countplot(\n",
    "    data=bankData, x=\"education\", hue=\"marital\",\n",
    "    palette=\"pastel\", edgecolor=\".6\",\n",
    "    ax=EM\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad2cfad",
   "metadata": {},
   "source": [
    "## Explore Attributes and Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726efd57",
   "metadata": {},
   "source": [
    "#### Identify and explain interesting relationships between features and the class you are trying to predict (i.e., relationships with variables and the target classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd007f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot stacked bar chart w/ totals above\n",
    "# stacked bar chart will be used for categorical variables\n",
    "# Will be comparing all significant variables to ProdTaken/ProductPitched with boxplots next\n",
    "def stacked_plot(x):\n",
    "    sns.set(palette='nipy_spectral')\n",
    "    tab1 = pd.crosstab(x,bankData['y'],margins=True)\n",
    "    df1 = pd.DataFrame(tab1)\n",
    "    df1['% Yes'] = round((df1.yes/df1.All)*100,2)\n",
    "    df1 = df1.sort_values(by='All', ascending=False)\n",
    "    print(df1)\n",
    "    print('-'*100)\n",
    "    tab = pd.crosstab(x,bankData['y'],normalize='index')\n",
    "    tab = tab.sort_values(by='no', ascending=True)\n",
    "    tab.plot(kind='bar',stacked=True,figsize=(15,5))\n",
    "    #plt.legend(loc='lower left', frameon=False)\n",
    "    #plt.legend(loc=\"upper left\", bbox_to_anchor=(0,1))\n",
    "    plt.legend(loc='upper right',bbox_to_anchor=(1.1, 1))\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136232cd",
   "metadata": {},
   "source": [
    "In reference to the table and plot below, there was evidence to suggest differences between levels for job. We saw that Retired subjects and Students were more likely to subscribe to a long term deposit, however their population size was drastically smaller. Practically speaking, it may make financial sense for these two groups to have had a higher rate for subscribing. This could be explained as neither having had a consistent and secure income, so both groups may have had a higher chance of seeking indirect income sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ed4537",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stacked_plot(bankData.job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49027122",
   "metadata": {},
   "source": [
    "There was evidence to suggest differences between levels depending on the month contacted. After normalizing the scale to get percentages of each month, we found that December was the highest rate of sign ups, however it also had the lowest number of calls made. Amount of calls made in December was significantly less than any other month. Future exploration is needed to investigate if that is an anomly due to few data points. Spring and fall specifically March, September and October had close to 50% that **subscribed a term deposit**. All other months had less than 20% sign up with exception of April just above that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f416bc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_plot(bankData.month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cb82bc",
   "metadata": {},
   "source": [
    "We continued the trend of plotting the subscription rates with the **contact**. We can see that subjects contacted via cellular devices are more likely to subscribe to a long term deposit at almost three times the rate (2.75). This could likely be due to the higher accessibility of cellular devices allowing a higher response rate based off the fact that cellular responses more than double that of the telephone responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf798af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_plot(bankData.contact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc6d242",
   "metadata": {},
   "source": [
    "There was visual evidence to suggest that subjects that had previously subscribed to long term loans were almost 3 times more likely to subscribe to another long term loan. However, the number of success responses was drastically lower than the rest of the data set. Given that long term deposits are financial descisions that more often rely on long term commitment, this makes practical sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14283a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stacked_plot(bankData.poutcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bcbccc",
   "metadata": {},
   "source": [
    "We then investigated continuous variables via distribution plots. We were unable to find any evidence of meaningful relationships that provide interpretibility not previously mentioned. However, the following cel provides the distribution plots for all continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b86035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(rc={\"figure.figsize\": (10, 7)})\n",
    "# for i in conCol:\n",
    "#     plt.figure(i)\n",
    "#     sns.displot(x=i, hue='y', data=bankData, kde=True ,fill=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b92934",
   "metadata": {},
   "source": [
    "## New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61953327",
   "metadata": {},
   "source": [
    "#### Are there other features that could be added to the data or created from existing features? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0106ae6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "We created new features via hot encoding the categorical variables. This was used to generate a heat map of the correlation between all variables including those that are originally categorical.\n",
    "\n",
    "The subject income was not given in the data set and it can be assumed that the response variable would have at least some level of correlation to a person's income. Along those same lines, knowing what the subject's **housing** or personal **loan** amount is could also provide valuable insight.\n",
    "\n",
    "\n",
    "\n",
    "Another interesting idea involves knowing if the method of **contact** changed from the previous **contact** with the potential client. This would give insight as to whether a calling someone on a house phone or cell phone would impact the response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a528f26",
   "metadata": {},
   "source": [
    "## Exceptional Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01da46f",
   "metadata": {},
   "source": [
    "#### You have free reign to provide additional analyses. One idea: implement dimensionality reduction, then visualize and interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a6a3a4",
   "metadata": {},
   "source": [
    "We feel that our exceptional work is provided through our heatmap analysis of the correlation between all the variables. Not only did we use a heatmap to plot the correlation, but we also used hot encoding on the categorical responses to help determine any colinearity between distinct levels of categorical levels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
