{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f07c2778",
   "metadata": {},
   "source": [
    "# Lab 2\n",
    "\n",
    "##### Team Members: Alex Lopez, Chris Haub, Erin McClure-Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f63f164",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KNeighborsClassifier\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#Suppress futurewarnings\u001b[39;00m\n\u001b[0;32m     30\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# Importing all packages and modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import glob\n",
    "import os\n",
    "import seaborn as sns #may not use\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Suppress futurewarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#Suppress convergence warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.simplefilter(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "#Set Figure Size\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcaedda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting up function to create confusion matrix\n",
    "def make_confusion_matrix(model,y_actual, x_test, labels=[1, 0]):\n",
    "    '''\n",
    "    model : classifier to predict values of X\n",
    "    y_actual : ground truth  \n",
    "    \n",
    "    '''\n",
    "    y_predict = model.predict(x_test)\n",
    "    cm=mt.confusion_matrix( y_actual, y_predict, labels=labels)\n",
    "    df_cm = pd.DataFrame(cm, index = [i for i in [\"Actual - No\",\"Actual - Yes\"]],\n",
    "                  columns = [i for i in ['Predicted - No','Predicted - Yes']])\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cm.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         cm.flatten()/np.sum(cm)]\n",
    "    labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n",
    "              zip(group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sns.heatmap(df_cm, annot=labels,fmt='')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dea960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_score(model,flag=True):\n",
    "    '''\n",
    "    model : classifier to predict values of X\n",
    "\n",
    "\n",
    "\n",
    "   '''\n",
    "    # defining an empty list to store train and test results\n",
    "    score_list=[]\n",
    "    \n",
    "    #Predicting on train and tests\n",
    "    pred_train = model.predict(xTrainScaled)\n",
    "    pred_test = model.predict(xTestScaled)\n",
    "    \n",
    "    #Accuracy of the model\n",
    "    train_acc = model.score(xTrainScaled,yTrain)\n",
    "    test_acc = model.score(xTestScaled,yTest)\n",
    "    \n",
    "    #Recall of the model\n",
    "    train_recall = mt.recall_score(yTrain,pred_train,pos_label=\"no\")\n",
    "    test_recall = mt.recall_score(yTest,pred_test,pos_label=\"no\")\n",
    "    \n",
    "    #Precision of the model\n",
    "    train_precision = mt.precision_score(yTrain,pred_train,pos_label=\"no\")\n",
    "    test_precision = mt.precision_score(yTest,pred_test,pos_label=\"no\")\n",
    "\n",
    "    \n",
    "    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision))\n",
    "        \n",
    "    # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.\n",
    "    if flag == True:\n",
    "        print(\"Accuracy on training set : \",model.score(xTrainScaled,yTrain))\n",
    "        print(\"Accuracy on test set : \",model.score(xTestScaled,yTest))\n",
    "        print(\"\\nRecall on training set : \",mt.recall_score(yTrain,pred_train,pos_label=\"no\"))\n",
    "        print(\"Recall on test set : \",mt.recall_score(yTest,pred_test,pos_label=\"no\"))\n",
    "        print(\"\\nPrecision on training set : \",mt.precision_score(yTrain,pred_train,pos_label=\"no\"))\n",
    "        print(\"Precision on test set : \",mt.precision_score(yTest,pred_test,pos_label=\"no\"))\n",
    "    \n",
    "    return score_list # returning the list with train and test scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3912e85b",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db728c1",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "***Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4828ca4",
   "metadata": {},
   "source": [
    "The first step was to load necessary packages and the dataset. In addition, we created a series of variables for indexing both the continuous and categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492e9c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Dataset from github repo\n",
    "bankData = pd.read_csv('bank-additional-full.csv', sep=';', na_values=\"unknown\")\n",
    "#Optional: Add in index column\n",
    "#bankData.insert(0, 'Sample_ID', range(1,len(bankData)+1))\n",
    "\n",
    "#Creating variables for indexing continuous and categorical variables\n",
    "conCol = ['age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate', \n",
    "          'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "\n",
    "catCol = ['job', 'marital', 'education', 'housing', 'loan', \n",
    "          'contact', 'month', 'day_of_week', 'poutcome'] # Default is removed from this list because it is not used "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c67bc1e",
   "metadata": {},
   "source": [
    "The next step was to drop entries with missing values, this removed 10,701 rows leaving 30,488 rows which, as discussed in previous assignments, was sufficient for the analysis requirements. In addition, we removed the 'default' column due to its consisting of almost entirely 'no' responses with a total of 3 'yes' responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8296d5b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bankData2 = bankData.dropna()\n",
    "#Remove 'default' column\n",
    "bankData2 = bankData2.drop(['default'], axis=1)\n",
    "bankData2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1857a7c8",
   "metadata": {},
   "source": [
    "Double checking the shape of the data showed that we had, indeed, a total of 30,488 rows and 20 attributes in our cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678904e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bankData2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3e23e1",
   "metadata": {},
   "source": [
    "We looked at the simple statistics of the continuous variables and noted that the means were close to the median, indicating that their distributions were fairly symmetrical and would not require any mathematical treatments prior to analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037546ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting basic statistical overview of continuous variables using the describe function\n",
    "bankData2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1b7fec",
   "metadata": {},
   "source": [
    "Looking at the target variable, y, we see that it is remarkably unbalanced with over 25,000 \"no\" responses and less than 5,000 \"yes\" responses. While we could balance the target variable to even out the responses, we felt that increasing the \"yes\" answers would bias the outcomes of the models. Since the \"no\" responses are real we chose not to balance the variable in order to obtain more realistic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bd655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='y', data = bankData2, kind = 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc0c1c4",
   "metadata": {},
   "source": [
    "Our second target variable, education, consists of seven levels: illiterate, basic 4, 6, and 9 years, highschool, university degree, and professional course. We noted that the majority of customers had a university degree (34%), followed by highschool (25%). The basic 9 year (14%) and professional course (14.1%) were fairly close followed by basic 4 year (8%) and then basic 6 year (5%). The illitterate level was 0.0004%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4887f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.catplot(x='education', data = bankData2, kind = 'count', height=7, aspect=11.7/8.27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6672317a",
   "metadata": {},
   "source": [
    "The code below returns the counts and percentages of each level in the categorical variables. For jobs, most customers are in admin (28%) followed by blue-collar (19%) and technician (18%). The marital status for the majority of customers is married (57%), followed by single (31%), and divorced (12%). There were more customers with university degrees (34%) than all other levels of education, while the those customers who had housing loans (54%) were close to those without housing loans (46%).\n",
    "\n",
    "From looking at the loan variable, we noted that only 16% of customers had taken out a personal loan, and from the contact variable we saw that the majority of customers used cellphones (67%) over telephones.\n",
    "\n",
    "Interestingly, significantly more customers were contacted in the month of May (32%) than any other month, though we cannot make any inferences about why this would be the case since it is likely due to the bank's protocols. The day of the week did not appear to have any influence on customer count, while the variable for outcome of the previous marketing campaign, poutcome, did show that the majority of customers had nonexistent outcomes (85%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d85f5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCP(column):\n",
    "    xCount = column.value_counts()\n",
    "    xPercentages = xCount/len(column)\n",
    "    outData = pd.DataFrame()\n",
    "    outData['Counts'] = xCount\n",
    "    outData['Percent of Total'] = xPercentages\n",
    "    return outData\n",
    "\n",
    "for i in catCol:\n",
    "    print(\"Percentages for \",i,\" : \\n\",getCP(bankData2[i]),\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee28989c",
   "metadata": {},
   "source": [
    "We performed a One Hot encoding on the categorical variables and stored the resulting dataset as a separate dataset before concatenating it with the continuous variables from the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f27f28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#One hot encoding of ALL categorical variables\n",
    "OneHotDF = pd.concat([pd.get_dummies(bankData2[col],prefix=col,drop_first=True) for col in catCol], axis=1)\n",
    "#Combining with continuous variables from cleaned dataset\n",
    "OneHotDF = pd.concat([bankData2.select_dtypes(exclude='object'),OneHotDF],axis=1)\n",
    "OneHotDF.head()\n",
    "\n",
    "#https://github.com/jakemdrew/DataMiningNotebooks/blob/master/01.%20Pandas.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f2bad1",
   "metadata": {},
   "source": [
    "The next step was to normalize and finalize preparing the dataset. We used SciKit's StandardScaler, which would fix any non-Gaussian curves to center around zero and force all standard deviations to one.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd45bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating X values\n",
    "x = OneHotDF\n",
    "xVal = x.values\n",
    "\n",
    "#Scaling values\n",
    "scl_obj = StandardScaler()\n",
    "xScaled = scl_obj.fit_transform(xVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c21f6c2",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "***Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1f7261",
   "metadata": {},
   "source": [
    "As previously stated, the final, cleaned, dataset we used had all rows with missing data removed removed mainly because the large size of the original data set allowed us to do so. This meant that rather than trying to impute categorical variables, we could simply remove those rows while still meeting our instance size requirements.\n",
    "\n",
    "After the missing variables were removed, we one-hot-encoded the categorical features so that we could work with only numeric features. This format is required for certain modeling algorithms that we planned to utilize for the analyses.\n",
    "\n",
    "Next, the data set was converted from a data frame to a list format. This was done in order to satisfy the cross validation training algorithim that requires this type of data input.\n",
    "\n",
    "Finally, the numeric, list format data set was scaled. This is required for many classifiers including, but not limited to, Random Forest and k-Nearest Neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd0422c",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2602296d",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "***Choose and explain your evaluation metrics that you will use (i.e., accuracy,\n",
    "precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357065e2",
   "metadata": {},
   "source": [
    "For the goal of predicting the target variable, \"y\", we chose to focus on precision because our goal was to minimize the number of false positives which, in turn, means minimizing the potential loss of funds spend towards marketing. This last point is especially important because, we would not want to direct a client to waste resources on customers who we predict will say \"yes\" to a loan when they will actually just say \"no\". Alternatively, if the marketing budget of a client was not a concern, focusing on recall would maximize potential customers saying \"yes\". In short, precision focuses on those customers who are most likely to choose \"yes\" to a short term loan, while recall focuses on maximizing potential returns. The reason why we chose precision over accuracy for this goal was because, as we previously showed, our target variable is unbalanced with approximately 87% \"no\" responses which infers that the accuracy of the model would be high but would also be biased towards over-predicting \"no\" responses.\n",
    "\n",
    "For the goal of using logistic regression to predict the level of education a customer has that will make them more likely to say \"yes\" to a loan we chose to use accuracy. Our reasoning for using accuracy was that the target variable in this case, education, was not greatly unbalanced with a reasonable differences between levels. Since we did not have to worry about unbalanced data, we chose accuracy because we wanted to know the percentage of correct predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7a6dde",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "***Choose the method you will use for dividing your data into training and\n",
    "testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why\n",
    "your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf353d",
   "metadata": {},
   "source": [
    "An important aspect of choosing our method of training/testing splitting was analyzing the balance of our target variable. We did so by comparing the number of responses of each class within the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b93d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCP(column):\n",
    "    xCount = column.value_counts()\n",
    "    xPercentages = xCount/len(column)\n",
    "    outData = pd.DataFrame()\n",
    "    outData['Counts'] = xCount\n",
    "    outData['Percent of Total'] = xPercentages\n",
    "    return outData\n",
    "\n",
    "print(\"Percentages for Target variable, Y: \\n\\n\",getCP(bankData2['y']),\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104773f9",
   "metadata": {},
   "source": [
    "As seen above, the target variable of our data set is imbalanced with ~87% \"no\" responses. With this imbalance in mind, we chose to proceed with stratified sampling which allows for a uniform distribution of the target variable between the training and test splits. In combination with this sampling strategy, we chose to perform stratified k-fold cross validation.\n",
    "\n",
    "The next step was to perform logistic regression on the newly created training and test sets...\n",
    "\n",
    "\n",
    "https://towardsdatascience.com/stratified-sampling-you-may-have-been-splitting-your-dataset-all-wrong-8cfdd0d32502"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca9ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "bankData.y.eq('yes').mul(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a7185a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "randomState = 777\n",
    "splits = 10\n",
    "cvStrat = StratifiedKFold(n_splits= splits, shuffle = True, random_state = randomState)\n",
    "\n",
    "lrClf = LogisticRegression(random_state=randomState)\n",
    "knn = KNeighborsClassifier(n_neighbors = 1)\n",
    "rf = RandomForestClassifier(random_state=randomState)\n",
    "xgb = XGBClassifier(random_state=randomState, eval_metric='logloss')\n",
    "\n",
    "y = bankData2['y']\n",
    "y = y.ravel()\n",
    "\n",
    "lrAccs = []\n",
    "knnAccs = []\n",
    "rfAccs = []\n",
    "xgbAccs = []\n",
    "\n",
    "labels = ['no', 'yes']\n",
    "\n",
    "for trainIndices, testIndices in cvStrat.split(xVal,y): \n",
    "    xTrain = xVal[trainIndices]\n",
    "    yTrain = y[trainIndices]\n",
    "    yTrainBin = yTrain\n",
    "    yTrainBin = (yTrainBin=='yes').astype(int)\n",
    "    \n",
    "    xTest = xVal[testIndices]\n",
    "    yTest = y[testIndices]\n",
    "    yTestBin = yTest\n",
    "    yTestBin = (yTestBin=='yes').astype(int)\n",
    "    \n",
    "    xTrainScaled = xScaled[trainIndices]\n",
    "    xTestScaled = xScaled[testIndices]\n",
    "    \n",
    "    #Scaled LR training\n",
    "    lrClf.fit(xTrainScaled, yTrain)\n",
    "    yHatLR = lrClf.predict(xTestScaled)\n",
    "    lrAccs.append(mt.accuracy_score(yTest, yHatLR))\n",
    "    \n",
    "    #Scaled KNN training\n",
    "    knn.fit(xTrainScaled, yTrain) \n",
    "    yHatKnn = knn.predict(xTestScaled)\n",
    "    knnAccs.append(mt.accuracy_score(yTest,yHatKnn))\n",
    "    \n",
    "    #Scaled XGBoost training\n",
    "    xgb.fit(xTrainScaled, yTrainBin)\n",
    "    yHatXgb = xgb.predict(xTestScaled)\n",
    "    xgbAccs.append(mt.accuracy_score(yTestBin, yHatXgb))\n",
    "    \n",
    "    #Scaled Random Forrest Training\n",
    "    yhat = np.zeros(y.shape)\n",
    "    rf.fit(xTrainScaled, yTrainBin)\n",
    "    yhat[testIndices] = rf.predict(xTestScaled)\n",
    "    rfAccs.append(rf.score(y, yhat))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e6176",
   "metadata": {},
   "source": [
    "The following code provides a visualization of Accuracy vs Split for the and non-scaled and scaled data. We see that the scaled data had increased accuracy over the non-scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140bc3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings(record=True):\n",
    "    normalPlot = plt.figure(1)\n",
    "    \n",
    "    knnAccs = pd.Series(data = knnAccs)\n",
    "    knnAccs.plot()\n",
    "    \n",
    "    lrAccs = pd.Series(data = lrAccs)\n",
    "    lrAccs.plot()\n",
    "    \n",
    "    rfAccs = pd.Series(data = rfAccs)\n",
    "    rfAccs.plot()\n",
    "    \n",
    "    xgbAccs = pd.Series(data = xgbAccs)\n",
    "    xgbAccs.plot()\n",
    "    \n",
    "    plt.xticks(np.arange(0, splits, 1))\n",
    "    plt.title(\"Accuracy of Different Models\")\n",
    "    plt.legend(loc=\"upper left\", labels = ['Knn', 'Logistic Regression', 'Random Forest','XGBoost'], prop={'size': 15})\n",
    "    \n",
    "    normalPlot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6270b941",
   "metadata": {},
   "source": [
    "Given the performance shown in the figure above, it was clear that the scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585a0a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics_score(lrClfScaled, flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41c8953",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_matrix(lrClf, yTest, xTestScaled, labels = ['no', 'yes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffdfe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_matrix(knn, yTest, xTestScaled, labels = ['no','yes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6411803d",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "***Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ef86ee",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c2461f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b567ec94",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5534cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(random_state=10, eval_metric='logloss')\n",
    "xgb.fit(xTrain,yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c168ef4b",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095ecc31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a765eb98",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "\n",
    "***Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75138842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75097d2a",
   "metadata": {},
   "source": [
    "### Part 5\n",
    "\n",
    "***Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniquesâ€”be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9f689b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5479830f",
   "metadata": {},
   "source": [
    "## Part 6\n",
    "\n",
    "***Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01034c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72933e7e",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "***How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06dcc3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49c47d47",
   "metadata": {},
   "source": [
    "## Exceptional Work\n",
    "\n",
    "You have free reign to provide additional analyses. One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
