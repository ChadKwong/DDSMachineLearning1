{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "883cc21d",
   "metadata": {},
   "source": [
    "# Mini Lab: SVM and Logistic Regression Modeling\n",
    "\n",
    "### Teammates: Chad Kwong, Erin McClure-Price, Alex Lopez, Chris Haub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846a4cf2",
   "metadata": {},
   "source": [
    "## Initial loading of dataset and all related packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426b95b2",
   "metadata": {},
   "source": [
    "The first step was to load necessary packages and the dataset. In addition, we created a series of variables for indexing both the continuous and categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b4b0040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import glob\n",
    "import os\n",
    "\n",
    "#Suppress futurewarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#Set Figure Size\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "#Import Dataset from github repo\n",
    "bankData = pd.read_csv('bank-additional-full.csv', sep=';', na_values=\"unknown\")\n",
    "#Optional: Add in index column\n",
    "#bankData.insert(0, 'Sample_ID', range(1,len(bankData)+1))\n",
    "\n",
    "#Creating variables for indexing continuous and categorical variables\n",
    "conCol = ['age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate', \n",
    "          'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "catCol = ['job', 'marital', 'education', 'default', 'housing', 'loan', \n",
    "          'contact', 'month', 'day_of_week', 'poutcome', 'y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4519835a",
   "metadata": {},
   "source": [
    "The next step was to drop entries with missing values, this removed #### rows leaving 30,488 rows for analysis. In addition, we removed the 'default' column because it consisted of entirely 'no' responses and a total of 3 'yes' responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30875409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>59</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age        job  marital            education default housing loan  \\\n",
       "0   56  housemaid  married             basic.4y      no      no   no   \n",
       "2   37   services  married          high.school      no     yes   no   \n",
       "3   40     admin.  married             basic.6y      no      no   no   \n",
       "4   56   services  married          high.school      no      no  yes   \n",
       "6   59     admin.  married  professional.course      no      no   no   \n",
       "\n",
       "     contact month day_of_week  ...  campaign  pdays  previous     poutcome  \\\n",
       "0  telephone   may         mon  ...         1    999         0  nonexistent   \n",
       "2  telephone   may         mon  ...         1    999         0  nonexistent   \n",
       "3  telephone   may         mon  ...         1    999         0  nonexistent   \n",
       "4  telephone   may         mon  ...         1    999         0  nonexistent   \n",
       "6  telephone   may         mon  ...         1    999         0  nonexistent   \n",
       "\n",
       "  emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
       "0          1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "2          1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "3          1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "4          1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "6          1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankData2 = bankData.dropna()\n",
    "#Remove 'default' column\n",
    "bankData2.drop(['default'], axis=1)\n",
    "bankData2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aa4dae",
   "metadata": {},
   "source": [
    "We double checked the dataset to ensure that all missing values had been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "384f6569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "job               0\n",
       "marital           0\n",
       "education         0\n",
       "default           0\n",
       "housing           0\n",
       "loan              0\n",
       "contact           0\n",
       "month             0\n",
       "day_of_week       0\n",
       "duration          0\n",
       "campaign          0\n",
       "pdays             0\n",
       "previous          0\n",
       "poutcome          0\n",
       "emp.var.rate      0\n",
       "cons.price.idx    0\n",
       "cons.conf.idx     0\n",
       "euribor3m         0\n",
       "nr.employed       0\n",
       "y                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checked for missing values, no missing values\n",
    "bankData2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd1f425",
   "metadata": {},
   "source": [
    "Checking the shape of the data showed that we had a total of 30,488 rows and 21 attributes in our cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f34dc6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30488, 21)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankData2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4164e88a",
   "metadata": {},
   "source": [
    "Before we could build a model, we needed to create a dataframe with the target variable, y, which was whether a customer decided to take on a long term deposit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "117f8b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    no\n",
       "2    no\n",
       "3    no\n",
       "4    no\n",
       "6    no\n",
       "Name: y, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define the target variable\n",
    "y = bankData2.y\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d270c7",
   "metadata": {},
   "source": [
    "The dataset is a mixture of both continuous, numeric, and categorical data, therefore we chose to separate the categorical attributes into a separate dataframe and apply One Hot Encoding to convert categorical entries to numeric data. The converted dataframe was concatenated back into the original dataframe and the original attributes were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "651f3b0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_blue-collar</th>\n",
       "      <th>job_entrepreneur</th>\n",
       "      <th>job_housemaid</th>\n",
       "      <th>job_management</th>\n",
       "      <th>job_retired</th>\n",
       "      <th>job_self-employed</th>\n",
       "      <th>job_services</th>\n",
       "      <th>job_student</th>\n",
       "      <th>job_technician</th>\n",
       "      <th>job_unemployed</th>\n",
       "      <th>...</th>\n",
       "      <th>month_dec</th>\n",
       "      <th>month_jul</th>\n",
       "      <th>month_jun</th>\n",
       "      <th>month_mar</th>\n",
       "      <th>month_may</th>\n",
       "      <th>month_nov</th>\n",
       "      <th>month_oct</th>\n",
       "      <th>month_sep</th>\n",
       "      <th>poutcome_nonexistent</th>\n",
       "      <th>poutcome_success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   job_blue-collar  job_entrepreneur  job_housemaid  job_management  \\\n",
       "0                0                 0              1               0   \n",
       "2                0                 0              0               0   \n",
       "3                0                 0              0               0   \n",
       "4                0                 0              0               0   \n",
       "6                0                 0              0               0   \n",
       "\n",
       "   job_retired  job_self-employed  job_services  job_student  job_technician  \\\n",
       "0            0                  0             0            0               0   \n",
       "2            0                  0             1            0               0   \n",
       "3            0                  0             0            0               0   \n",
       "4            0                  0             1            0               0   \n",
       "6            0                  0             0            0               0   \n",
       "\n",
       "   job_unemployed  ...  month_dec  month_jul  month_jun  month_mar  month_may  \\\n",
       "0               0  ...          0          0          0          0          1   \n",
       "2               0  ...          0          0          0          0          1   \n",
       "3               0  ...          0          0          0          0          1   \n",
       "4               0  ...          0          0          0          0          1   \n",
       "6               0  ...          0          0          0          0          1   \n",
       "\n",
       "   month_nov  month_oct  month_sep  poutcome_nonexistent  poutcome_success  \n",
       "0          0          0          0                     1                 0  \n",
       "2          0          0          0                     1                 0  \n",
       "3          0          0          0                     1                 0  \n",
       "4          0          0          0                     1                 0  \n",
       "6          0          0          0                     1                 0  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One hot encoding of ALL categorical variables\n",
    "\n",
    "# pd.concat([*]], axis=1) // this line of code concatenates all the data frames in the [*] list\n",
    "# [** for col in categ_features] // this steps through each feature in categ_features and \n",
    "#                                //   creates a new element in a list based on the output of **\n",
    "# pd.get_dummies(df_imputed[col],prefix=col) // this creates a one hot encoded dataframe of the variable=col (like code above)\n",
    "\n",
    "categ_features = ['job','marital','education','default','housing','loan','contact','month','poutcome'];\n",
    "\n",
    "OneHotDF = pd.concat([pd.get_dummies(bankData2[col],prefix=col,drop_first=True) for col in categ_features], axis=1)\n",
    "\n",
    "OneHotDF.head()\n",
    "\n",
    "#https://github.com/jakemdrew/DataMiningNotebooks/blob/master/01.%20Pandas.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00bc9d1",
   "metadata": {},
   "source": [
    "# Creating and Adjusting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e3f36b",
   "metadata": {},
   "source": [
    "**Create a logistic regression model and a support vector machine model for the\n",
    "classification task involved with your dataset. Assess how well each model performs (use\n",
    "80/20 training/testing split for your data). Adjust parameters of the models to make them more\n",
    "accurate. If your dataset size requires the use of stochastic gradient descent, then linear kernel\n",
    "only is fine to use.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49cbeb4",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c48988",
   "metadata": {},
   "source": [
    "Before building the models, we first separated out the Features and the Target for establishing testing and training sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5dabda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating out the features for training and testing\n",
    "X = pd.concat([bankData2.select_dtypes(exclude='object'),OneHotDF],axis=1)\n",
    "X = X.values\n",
    "\n",
    "#Separating out target for training and testing\n",
    "Y = bankData2.pop(\"y\")\n",
    "Y = Y.values.ravel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39d9575",
   "metadata": {},
   "source": [
    "Once the features were separated, we performed hold out cross validation with 3 splits using a basic Logistic Regression model. We then compared accuracies between iterations of cross validation performed on the test and training splits. The random state was set to 1 for replicability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf163b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 1  ====\n",
      "accuracy: 0.9022630370613316\n",
      "confusion matrix\n",
      " [[5172  164]\n",
      " [ 432  330]]\n",
      "====Iteration 2  ====\n",
      "accuracy: 0.9050508363397836\n",
      "confusion matrix\n",
      " [[5188  157]\n",
      " [ 422  331]]\n",
      "====Iteration 3  ====\n",
      "accuracy: 0.8934076746474254\n",
      "confusion matrix\n",
      " [[5099  170]\n",
      " [ 480  349]]\n"
     ]
    }
   ],
   "source": [
    "#Cross validation, using 80/20 train/test splitting\n",
    "splits = 3\n",
    "randomState=1\n",
    "cv = ShuffleSplit(n_splits=splits, test_size=0.20, random_state=randomState)\n",
    "\n",
    "#Creating standard scaler object\n",
    "scl_obj = StandardScaler()\n",
    "\n",
    "#Creating Logistic Regression object\n",
    "lr_clf = LogisticRegression(random_state=randomState)\n",
    "\n",
    "#Setting Iteration start point for for loop\n",
    "iter_num=1\n",
    "\n",
    "#For loop for creating testing and training sets and running a full basic LR model to\n",
    "#Compare iterations the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv.split(X,Y): \n",
    "    X_train = X[train_indices]\n",
    "    Y_train = Y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    Y_test = Y[test_indices] \n",
    "    \n",
    "    X_train_scaled = scl_obj.fit_transform(X_train) # apply to training\n",
    "    X_test_scaled = scl_obj.transform(X_test) \n",
    "    \n",
    "    lr_clf.fit(X_train_scaled, Y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "    acc = mt.accuracy_score(Y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(Y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print('accuracy:', acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    #print(conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "#Also note that every time you run the above code\n",
    "#it randomly creates a new training and testing set, \n",
    "#so accuracy will be different each time\n",
    "    \n",
    "\n",
    "#https://github.com/jakemdrew/DataMiningNotebooks/blob/master/04.%20Logits%20and%20SVM.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d762358f",
   "metadata": {},
   "source": [
    "We can see from above that the resulting models tended to hover around 90% accuracy. The next step was for us to hyper tune the parameters of our logistic regression model using the following interactive code. Accuracy values will vary from above due to slight differences the training and test sets generated from the hold out cross validation. Our focus is on what changes to parameters improve the general model, so these small differences do not have an impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4a17942",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Parameter Tuning:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffbd4d4ffc94d329c72b0c0ced2039e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='penMethod', options=('l2', 'l1', 'elasticnet', 'none'), value='l2'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.exploreLogisticRegression(penMethod, cost, algorithm, maxIterations, dualOption, fiOption, iScaling, tolerance)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we can change some of the parameters interactively\n",
    "from ipywidgets import widgets as wd\n",
    "\n",
    "def exploreLogisticRegression(penMethod, cost, algorithm, maxIterations, dualOption, fiOption, iScaling,  tolerance):\n",
    "    model = LogisticRegression(random_state=randomState,\n",
    "                               penalty='l2',\n",
    "                               C=cost,\n",
    "                               solver=algorithm,\n",
    "                               max_iter = maxIterations,\n",
    "                               dual = dualOption,\n",
    "                               fit_intercept = fiOption,\n",
    "                               intercept_scaling = iScaling,\n",
    "                               tol = tolerance * 1e-4)\n",
    "    acc = cross_val_score(model,X,y=y,cv=cv)\n",
    "    print(\"\\n\\nUsing the settings above, the accuracy is\",acc[0])\n",
    "\n",
    "print('Logistic Regression Parameter Tuning:')\n",
    "#Default values:\n",
    "#PenMethod = l2, cost = 1, algorithm = lbfgs, maxIterations = 100\n",
    "#verboseN = 0, dualOption = False, tolerance = 1e-4 or 1 on the sliders,\n",
    "#\n",
    "wd.interact(exploreLogisticRegression,\n",
    "            penMethod=['l2','l1','elasticnet', 'none'],\n",
    "            cost=(0.001,5,0.05),\n",
    "            algorithm = ['liblinear', 'saga', 'sag', 'newton-cg', 'lbfgs'],\n",
    "            maxIterations = (50,500,10),\n",
    "            dualOption = [False, True],\n",
    "            fiOption = [True,False],\n",
    "            iScaling = (1,20,1),\n",
    "            tolerance = (1,10,1),\n",
    "            __manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc72e28",
   "metadata": {},
   "source": [
    "From our fine tuning above, we found that the only parameter that seemed to noticably affect our model was the C parameter, which is the value of the inverse of the regularization strength. The higher the C value, the less regularization is applied to reduce generalization errors opposed to training errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a201828",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-89053c8e94cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0maccs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "for i in range(1,20):\n",
    "    model = LogisticRegression(C=i * .05, solver = 'liblinear')\n",
    "    model.fit(X_train_scaled, Y_train)\n",
    "    preds = model.predict(X_test_scaled)\n",
    "    accs.append(mt.accuracy_score(Y_test,preds))\n",
    "\n",
    "accs = pd.Series(accs)\n",
    "accs.plot()\n",
    "print('The best C value for this model is', accs.idxmax() * .05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92af2abe",
   "metadata": {},
   "source": [
    "From our resulting plot above, we can see that the optimal value for C is about .3. Once our model was tuned, we constructed a full model to generate the accuracy and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bafe019",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "finalModel = LogisticRegression(penalty='l2', C=0.3, solver='liblinear')\n",
    "finalModel.fit(X_train_scaled,Y_train)\n",
    "preds = finalModel.predict(X_test_scaled)\n",
    "lrAcc = mt.accuracy_score(Y_test,preds)\n",
    "conf = mt.confusion_matrix(Y_test,preds)\n",
    "print('accuracy:', lrAcc )\n",
    "print(\"Confusion Matrix:\\n\",conf,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944e1bad",
   "metadata": {},
   "source": [
    "We obtained an overal accuracy score of 89.34% and it took approximately half a second to run (.445 ms)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6881c8",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a20f21f",
   "metadata": {},
   "source": [
    "After obtaining our final Logistic Regression model, we constructed a SVM model using the SVC object from Scikit-learn. Our dataset was already scaled from our previous model, so we did not need to do any further transformations to our dataset befor constructing the model. Because our training set contained a small number of predictors but a larger size of individual obersvations (43x24390), we chose to use a linear kernal approach to building the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78605401",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Training the model\n",
    "svm_clf = SVC(C=0.5, kernel='linear', degree=3, gamma='auto')\n",
    "svm_clf.fit(X_train_scaled, Y_train)\n",
    "\n",
    "#Generating Predictions\n",
    "y_hatSVM = svm_clf.predict(X_test_scaled)\n",
    "\n",
    "#Calculating accuracy and confusion matrix and printing the two\n",
    "svmAcc = mt.accuracy_score(Y_test,y_hatSVM)\n",
    "conf = mt.confusion_matrix(Y_test,y_hatSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3eeec8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Printing information above\n",
    "print('accuracy:', svmAcc )\n",
    "print(\"Confusion Matrix:\\n\",conf,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1fa28c",
   "metadata": {},
   "source": [
    "From our initial SVM model, we obtained an accuracy score of 88.49% and it took 14.7 seconds to generate. With this in mind, we attempted to make improvements to the model. We attempted to make adjustments to the parameters within the model object to raise our accuracy, but no matter what approach we took, our accuracy would not improve. However, we did find that as we decreased the C parameter, the performance time of the model improved with minimal loss in accuracy. C is proportional to the inverse of the regulization of the model, and therefore as we decrease C, the regularity of the model increases. We can expect an increase in regularity to decrease the performance time as well as the model is essentially limiting itself from overfitting the model. \n",
    "\n",
    "The next four cells present models with different C values that vary in decreasing magnitude. Using the python time function, we calculated the time to execute each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f2f665",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "model = SVC(C=.5, kernel = 'linear', degree=3, gamma='auto')\n",
    "model.fit(X_train_scaled, Y_train)\n",
    "preds = model.predict(X_test_scaled)\n",
    "mt.accuracy_score(Y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9d4487",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "model = SVC(C=.05, kernel = 'linear', degree=3, gamma='auto')\n",
    "model.fit(X_train_scaled, Y_train)\n",
    "preds = model.predict(X_test_scaled)\n",
    "mt.accuracy_score(Y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc701d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "model = SVC(C=.005, kernel = 'linear', degree=3, gamma='auto')\n",
    "model.fit(X_train_scaled, Y_train)\n",
    "preds = model.predict(X_test_scaled)\n",
    "mt.accuracy_score(Y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb39f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "model = SVC(C=.001, kernel = 'linear', degree=3, gamma='auto')\n",
    "model.fit(X_train_scaled, Y_train)\n",
    "preds = model.predict(X_test_scaled)\n",
    "mt.accuracy_score(Y_test,preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4829df",
   "metadata": {},
   "source": [
    "From the results, we can see that our performance time nearly halved while our accuracy suffered by less than 1%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c1e0c",
   "metadata": {},
   "source": [
    "## Gradient Based Alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c980de",
   "metadata": {},
   "source": [
    "The group discussed the option of using the stochastic gradient descent (SGD) model technique. However, it was determined that since the data set in this analysis is relatively small, having only 44 features and around 32,000 entries and the run times of our models were not extensive, using the SGD optimizer did not seem like a critical model to perform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba23aad0",
   "metadata": {},
   "source": [
    "# Discussing Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623dc23d",
   "metadata": {},
   "source": [
    "**Discuss the advantages of each model for each classification task. Does one type\n",
    "of model offer superior performance over another in terms of prediction accuracy? In terms of\n",
    "training time or efficiency? Explain in detail.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b70bef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Converting target to binary for log loss score\n",
    "yBinary = pd.Series(np.where(Y == 'yes', 1, 0),y.index)\n",
    "\n",
    "#Converting predictions to binary for log loss score\n",
    "yhatSVMBinary = pd.Series(np.where(y_hatSVM == 'yes', 1, 0))\n",
    "yhatLRBinary = pd.Series(np.where(preds == 'yes', 1, 0))\n",
    "\n",
    "#Calculating log loss scores for both models displayed above\n",
    "logLossSVM = log_loss(Y[test_indices],yhatSVMBinary)\n",
    "logLossLR = log_loss(Y[test_indices],yhatLRBinary)\n",
    "print('The Log Loss of the logistic regression model is: ', logLossLR)\n",
    "print('The Log Loss of the SVM model is: ', logLossSVM)\n",
    "#Displaying percent difference between the two scores\n",
    "percDif = str(round(abs((logLossLR - logLossSVM)/logLossLR)*100,2))\n",
    "print('The difference between the two scores is',percDif,'%')\n",
    "\n",
    "#Printing out accuracies of both models\n",
    "print('\\nThe Accuracy of the logistic regression model is: ', lrAcc)\n",
    "print('The Accuracy of the SVM model is: ', svmAcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4472d1d",
   "metadata": {},
   "source": [
    "When it comes to log-loss scores, the model with the lower score is superior. Log loss is an indication of how close the prediction probability is to the actual value. As the probability diverges from the actual value, the log loss score increases. With this logic, we can see that the logistic regression model performs better by less than 5% (3.68159 < 3.84586) when it comes to full models. We also consistently obtained slightly higher accuracy scores through logistic regression (89.3% vs 88.4%). In terms of performance, we also saw advantages to the logistic regression model. Our logistic regression model took less than a second to run while we saw performance times as low as 8 seconds with the SVM model. With this in mind, we suggest the application of a logistic regression model over a SVM model for this specific classification problem.\n",
    "\n",
    "This makes practical sense as both models are utilizing the dataset that is one hot encoded to remove classiciation type predictors. If we were to compare the two models using the non-encoded data set, we would have expected the SVM model to display a better Log Loss score because SVM models are ideal for multiclass approaches to classification. The dataset utilized in these models is already structured with identified independent variables and SVM is known to perform better on data sets that are less structured, so there is further evidence to suggest that our dataset is better utilized through a logistic regression model. \n",
    "\n",
    "It is important to note that while we saw slightly better performance in our logistic regression model, SVM models are less likely to overfit. However, this overfitting may be significantly decreased through adjustments of the C parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3d3232",
   "metadata": {},
   "source": [
    "**Reference: https://medium.com/axum-labs/logistic-regression-vs-support-vector-machines-svm-c335610a3d16#:~:text=SVM%20works%20well%20with%20unstructured,is%20based%20on%20statistical%20approaches.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5659d85d",
   "metadata": {},
   "source": [
    "# Interpretting weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342888fb",
   "metadata": {},
   "source": [
    "**Use the weights from logistic regression to interpret the importance of different\n",
    "features for each classification task. Explain your interpretation in detail. Why do you think\n",
    "some variables are more important?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded42bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing weights on a normalized scale\n",
    "print('The following is a breakdown of the coefficient weights:\\n')\n",
    "#Sort these attributes and spit them out\n",
    "zip_vars = zip(abs(finalModel.coef_.T),pd.concat([bankData2.select_dtypes(exclude='object'),OneHotDF],axis=1).columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars, reverse=True)\n",
    "\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out\n",
    "#Plot magnitude of weights in descending order\n",
    "#Grabbing column names for labels\n",
    "#columns = pd.concat([bankData2.select_dtypes(exclude='object'),OneHotDF],axis=0).columns\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#Set Figure Size\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "#Plotting weights\n",
    "labels = pd.concat([bankData2.select_dtypes(exclude='object'),OneHotDF],axis=1).columns\n",
    "weights = abs(pd.Series(finalModel.coef_[0],index=labels)).sort_values(ascending=False)\n",
    "weights.plot(kind='bar', title=\"Absolute Value of Weight Coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8931282",
   "metadata": {},
   "source": [
    "From the plot above, we can visually determine that there are only 3 features that have a magnitude weight above 1: emp.var.rate, duration, and cons.price.idx. Of these three variables, emp.var.rate and cons.price.idx are variables that do not vary between different people. Instead they are statistics that are updated every quarter. the euribor3m variable also shares this characteristic, although it is an international statistic that measures the interest rates of European banks. While these variables may present some inbalance to the model, they also present evidence that the likelyhood of a client subscribing to a term deposit may depend heavily on the socio-economic status of the country as well as the length the campaign to get the client to subscribe. \n",
    "\n",
    "With heavier weights on variables that are more time dependent as displayed above, there evidence that a time series model could actually perform better for this specific classification problem. Furthermore, variables such as month_aug and month_mar are one hot encoded variables that denote the time of month of the last point of contact. This is further evidence to suggest that a time series model may be appropriate for this kind of classification problem With this information we would fully suggest that term deposit subscription rate be investigated in a time series setting in addition to using classification. \n",
    "\n",
    "The other variable that has a weight above 1 is duration. This variable denotes the length of the point of last contact with the prospect subscriber. This variable could not be practically used to predict whether a new client will subscribe to a term deposit because one should expect the length of the call to be longer for those customers who decided to opt for the bank term deposit, while those who did not take out a deposit would not stay on the call for as long; therefore, the duration has a direct effect on the outcome of the model but is not a beneficial attribute for creating accurate predictions. However, it can be used as evidence to suggest that the longer you are in contact with a prospective client, the more likely they are to subscribe.\n",
    "\n",
    "In terms of interesting observations, one thing we noticed was how people who were contacted via a non-cellular phone were more likely to subscribe to a term deposit. We also observed a higher chance of a person subscribing if they graduated with a university degree or worked in blue collar or service jobs, however the significance to the model for all of these variables is minor with coefficient values less than .5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1936084e",
   "metadata": {},
   "source": [
    "# SVM Vectors Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ef5f4c",
   "metadata": {},
   "source": [
    "**Look at the chosen support vectors for the classification task. Do these provide\n",
    "any insight into the data? Explain.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a42896f",
   "metadata": {},
   "source": [
    "We decided to investigate our SVM vectors generated by the SVC package through kernal density estimate plots. We generated plots for both the original dataset and the dataset with the selected indices that were used by the support vectors before making comparisons. In addition to this, we also displayed the shape, indices, and numbers of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113e0db0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Make a dataframe of the training data\n",
    "df_tested_on = bankData2.iloc[train_indices].copy()\n",
    "\n",
    "#Getting the support vectors from the trained model\n",
    "df_support = bankData2.iloc[svm_clf.support_,:].copy()\n",
    "\n",
    "#Adding y column back into datasets\n",
    "df_support[\"y\"] = Y[svm_clf.support_] \n",
    "bankData2[\"y\"] = Y # also add it back in for the original data\n",
    "\n",
    "#Group the original data and the support vectors\n",
    "df_grouped_support = df_support.groupby([\"y\"])\n",
    "df_grouped = bankData2.groupby([\"y\"])\n",
    "\n",
    "#Plotting Kernal Density Estimates of all continuous variables\n",
    "for v in bankData[conCol].columns:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    # plot support vector stats\n",
    "    plt.subplot(1,2,1)\n",
    "    ax = df_grouped_support[v].plot.kde() \n",
    "    plt.legend([\"n\",\"y\"])\n",
    "    plt.title(v+' (Instances chosen as Support Vectors)')\n",
    "    \n",
    "    # plot original distributions\n",
    "    plt.subplot(1,2,2)\n",
    "    ax = df_grouped[v].plot.kde() \n",
    "    plt.legend(['y','n'])\n",
    "    plt.title(v+' (Original)')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d66a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing Support Vector shapes and information\n",
    "print(\"The shape of the support vectors is \",svm_clf.support_vectors_.shape)\n",
    "print(\"The indices of support vectors are\",svm_clf.support_.shape)\n",
    "print(\"The numbers of support vectors for each class are\",svm_clf.n_support_, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4500688",
   "metadata": {},
   "source": [
    "The chosen support vectors appear to generally show less separation relative to the original data set. This is because the support vectors will usually select data points on the edge of the class boundaries. However, for certain features, like age, the change in separation is a more difficult to notice. Having performed a detailed EDA in the last lab, we believe less change in separation is likely due to the similarity in distributions between the features and the response classes. Looking at the features that are showing a more drastic change in separation, like nr.employed (number of employees), we see that density plot distributions and peaks are fairly similar and track each other to some degree. This is to be expected because the support vector distributions should reflect the population from which they are being pulled from and again, because the support vector instances are typically on the edge of the class boundaries, the response class density distributions will inevitably be more similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5103417",
   "metadata": {},
   "source": [
    "## Start Project 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e06ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.get_dummies(bankData2[\"y\"],prefix=\"y\", drop_first=True)\n",
    "Y = Y.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc867b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510f9aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288eda23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# # create variables we are more familiar with\n",
    "yhat = np.zeros(Y.shape) # we will fill this with predictions\n",
    "\n",
    "scl = StandardScaler()\n",
    "X = scl.fit_transform(X)\n",
    "\n",
    "# # setup pipeline to take PCA, then fit a KNN classifier\n",
    "# ## Randomized used to run faster\n",
    "# clf_pipe = Pipeline(\n",
    "#     [('PCA_Eric',PCA(n_components=43,svd_solver='randomized'))]\n",
    "# )\n",
    "\n",
    "clf_pipe = Pipeline(\n",
    "    [('PCA_Eric',PCA(n_components=43,svd_solver='randomized')),\n",
    "     ('CLF_Eric',KNeighborsClassifier(n_neighbors=1))]\n",
    ")\n",
    "\n",
    "# # now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv.split(X,Y):\n",
    "    clf_pipe.fit(X[train],Y[train])\n",
    "    yhat[test] = clf_pipe.predict(X[test])\n",
    "    \n",
    "\n",
    "\n",
    "total_accuracy = mt.accuracy_score(Y, yhat)\n",
    "print ('KNN, pipeline accuracy', total_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe87727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_class_accuracy(ytrue,yhat):\n",
    "    conf = mt.confusion_matrix(ytrue,yhat)\n",
    "    norm_conf = conf.astype('float') / conf.sum(axis=1)[:, np.newaxis]\n",
    "    return np.diag(norm_conf)\n",
    "\n",
    "def plot_class_acc(ytrue,yhat, title=''):\n",
    "    acc_list = per_class_accuracy(ytrue,yhat)\n",
    "    plt.bar(range(len(acc_list)), acc_list)\n",
    "    plt.xlabel('Class value (one per face)')\n",
    "    plt.ylabel('Accuracy within class')\n",
    "    plt.title(title+\", Total Acc=%.1f\"%(100*mt.accuracy_score(ytrue,yhat)))\n",
    "    plt.grid()\n",
    "    plt.ylim([0,1])\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "plot_class_acc(Y,yhat,title=\"KNN\")\n",
    "\n",
    "per_class_accuracy(Y,yhat)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d45bb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "## Image related data \n",
    "clf_pipe = Pipeline(\n",
    "    [('PCA',PCA(n_components=43, svd_solver='randomized')),\n",
    "     ('CLF',RandomForestClassifier(max_depth=50, n_estimators=150, n_jobs=-1))]\n",
    ")\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv.split(X,y):\n",
    "    clf_pipe.fit(X[train],Y[train])\n",
    "    yhat[test] = clf_pipe.predict(X[test])\n",
    "    \n",
    "total_accuracy = mt.accuracy_score(Y, yhat)\n",
    "print ('Pipeline accuracy', total_accuracy)\n",
    "plot_class_acc(Y,yhat,title=\"Random Forest + PCA\")\n",
    "\n",
    "# added yhat_score for ROC curve\n",
    "yhat_score = np.zeros((y.shape[0],2))\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "## predict_proba  returns a vector \n",
    "for train, test in cv.split(X,y):\n",
    "    clf_pipe.fit(X[train],y[train])\n",
    "    yhat[test] = clf_pipe.predict(X[test])\n",
    "    yhat_score[test] = clf_pipe.predict_proba(X[test])\n",
    "\n",
    "total_accuracy = mt.accuracy_score(y, yhat)\n",
    "print ('Pipeline accuracy', total_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750accea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_palette(\"dark\")\n",
    "# code manipulated from http://scikit-learn.org/stable/auto_examples/plot_roc.html\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Compute ROC curve for a subset of interesting classes\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in np.unique(y):\n",
    "    fpr[i], tpr[i], _ = mt.roc_curve(y, yhat_score[:, i], pos_label=i)\n",
    "    roc_auc[i] = mt.auc(fpr[i], tpr[i])\n",
    "    \n",
    "\n",
    "print(roc_auc)\n",
    "\n",
    "plt.plot(fpr[1], tpr[1])\n",
    "\n",
    "\n",
    "plt.legend(loc=\"lower right\")  \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c516c87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "## Out of bag score, unique to random forest, will take leftover records and feed them through\n",
    "## model and score them \n",
    "clf = RandomForestClassifier(max_depth=50, n_estimators=150, n_jobs=-1, oob_score=True)\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv.split(X,Y):\n",
    "    clf.fit(X[train],Y[train])\n",
    "    yhat[test] = clf.predict(X[test])\n",
    "    \n",
    "total_accuracy = mt.accuracy_score(Y, yhat)\n",
    "print ('Accuracy', total_accuracy)\n",
    "plot_class_acc(Y,yhat,title=\"Random Forest, Raw\")\n",
    "\n",
    "\n",
    "# added yhat_score for ROC curve\n",
    "yhat_score = np.zeros((y.shape[0],2))\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "## predict_proba  returns a vector \n",
    "for train, test in cv.split(X,y):\n",
    "    clf_pipe.fit(X[train],y[train])\n",
    "    yhat[test] = clf_pipe.predict(X[test])\n",
    "    yhat_score[test] = clf_pipe.predict_proba(X[test])\n",
    "\n",
    "total_accuracy = mt.accuracy_score(y, yhat)\n",
    "print ('Pipeline accuracy', total_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b951e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_palette(\"dark\")\n",
    "# code manipulated from http://scikit-learn.org/stable/auto_examples/plot_roc.html\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Compute ROC curve for a subset of interesting classes\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in np.unique(y):\n",
    "    fpr[i], tpr[i], _ = mt.roc_curve(y, yhat_score[:, i], pos_label=i)\n",
    "    roc_auc[i] = mt.auc(fpr[i], tpr[i])\n",
    "    \n",
    "    \n",
    "\n",
    "print(roc_auc)\n",
    "\n",
    "plt.plot(fpr[1], tpr[1])\n",
    "\n",
    "plt.legend(loc=\"lower right\")  \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce12d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets get access to the different properties of our RF\n",
    "\n",
    "print (clf)\n",
    "\n",
    "plt.barh(range(len(clf.feature_importances_)), clf.feature_importances_)\n",
    "plt.show()\n",
    "\n",
    "print ('Generalization score estimate from training data', clf.oob_score_)\n",
    "\n",
    "# Most importatn variable is duration\n",
    "# X_View = pd.concat([bankData2.select_dtypes(exclude='object'),OneHotDF],axis=1)\n",
    "# X_View.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9cf28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tree Ensemble Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630f1f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "num_estimators = 50\n",
    "# lets train some trees\n",
    "clf_array = [\n",
    "    ('Stump',              DecisionTreeClassifier(max_depth=1, min_samples_leaf=1)),\n",
    "    ('Tree',               DecisionTreeClassifier()),\n",
    "    ('Random Trees',       RandomForestClassifier(max_depth=50, n_estimators=num_estimators)),\n",
    "    ('Extra Random Trees', ExtraTreesClassifier(n_estimators=num_estimators,min_samples_split=2)),\n",
    "    ('Boosted Tree',       GradientBoostingClassifier(n_estimators=num_estimators)), #takes a long time\n",
    "    ]\n",
    "\n",
    "for clf in clf_array:\n",
    "    acc = cross_val_score(clf[1],X,y,cv=3)\n",
    "    print (clf[0], acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ef410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# setup pipeline to take PCA, then fit a different classifier\n",
    "clf_pipe = Pipeline(\n",
    "    [('PCA',PCA(n_components=43,svd_solver='randomized')),\n",
    "     ('CLF',GaussianNB())]\n",
    ")\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv.split(X,Y):\n",
    "    clf_pipe.fit(X[train],Y[train])\n",
    "    yhat[test] = clf_pipe.predict(X[test])\n",
    "\n",
    "total_accuracy = mt.accuracy_score(Y, yhat)\n",
    "print ('Pipeline accuracy', total_accuracy)\n",
    "plot_class_acc(Y,yhat,title=\"Naive Bayes + PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bb2353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# May not use this as is frequency\n",
    "\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "y = Y\n",
    "\n",
    "freq_infreq_threshold = 40\n",
    "\n",
    "# get various measures of performance\n",
    "total_accuracy = mt.accuracy_score(y, yhat)\n",
    "per_class_acc_list = per_class_accuracy(y,yhat)\n",
    "\n",
    "prec_for_freq_classes = []\n",
    "recall_for_infreq_classes = []\n",
    "rec_tot = []\n",
    "prec_tot = []\n",
    "\n",
    "for cls in np.unique(y):\n",
    "    idx = (y==cls) # get classes\n",
    "    ytmp_actual = np.zeros(y.shape) # make binary class problem\n",
    "    ytmp_actual[idx] = 1 # set the instances for this specific class\n",
    "    \n",
    "    ytmp_predicted = np.zeros(y.shape) # binary prediction array\n",
    "    ytmp_predicted[yhat==cls] = 1\n",
    "    \n",
    "    num_in_class = sum(idx)\n",
    "    \n",
    "    rec = mt.recall_score(ytmp_actual, ytmp_predicted)\n",
    "    prec = mt.precision_score(ytmp_actual, ytmp_predicted)\n",
    "    rec_tot.append(rec)\n",
    "    prec_tot.append(prec)\n",
    "    \n",
    "    if num_in_class < freq_infreq_threshold:\n",
    "        recall_for_infreq_classes.append(rec)\n",
    "    elif num_in_class >= freq_infreq_threshold:\n",
    "        prec_for_freq_classes.append(prec)\n",
    "        \n",
    "\n",
    "## Need to change titles, uses frequency        \n",
    "print ('Total Accuracy:',total_accuracy)\n",
    "print ('Number of infrequent faces:',len(recall_for_infreq_classes), \n",
    "       'with average recall of:', np.mean(recall_for_infreq_classes))\n",
    "print ('Number of frequent faces:',len(prec_for_freq_classes), \n",
    "       'with average precision of:',np.mean(prec_for_freq_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d0a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_score = np.zeros((y.shape[0],2))\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "## predict_proba  returns a vector \n",
    "for train, test in cv.split(X,y):\n",
    "    clf_pipe.fit(X[train],y[train])\n",
    "    yhat[test] = clf_pipe.predict(X[test])\n",
    "    yhat_score[test] = clf_pipe.predict_proba(X[test])\n",
    "\n",
    "total_accuracy = mt.accuracy_score(y, yhat)\n",
    "print ('Pipeline accuracy', total_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d0d65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_palette(\"dark\")\n",
    "# code manipulated from http://scikit-learn.org/stable/auto_examples/plot_roc.html\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Compute ROC curve for a subset of interesting classes\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in np.unique(y):\n",
    "    fpr[i], tpr[i], _ = mt.roc_curve(y, yhat_score[:, i], pos_label=i)\n",
    "    roc_auc[i] = mt.auc(fpr[i], tpr[i])\n",
    "    \n",
    "plt.plot(fpr[1], tpr[1])\n",
    "\n",
    "\n",
    "plt.legend(loc=\"lower right\")  \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e314b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#roc_auc\n",
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867352f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be4e08b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
