{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff687ea3",
   "metadata": {},
   "source": [
    "# Lab 3\n",
    "\n",
    "### Team Members:\n",
    " - Alex Lopez\n",
    " - Chris Haub\n",
    " - Erin McClure-Price\n",
    " - Chad Kwong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3f66b8",
   "metadata": {},
   "source": [
    "#### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a725e35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "#Import Scaling Libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "import plotly.express as px #EMP used for some outlier stuff\n",
    "from scipy import stats #EMP used for some outlier stuff\n",
    "\n",
    "#KMeans and evaluation metrics\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import k_means\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "#additional Clustering libraries\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage,cophenet\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import SpectralClustering \n",
    "from kmodes.kmodes import KModes\n",
    "from kmodes.kmodes import KModes\n",
    "\n",
    "#Importing random library for different options of setting seeds\n",
    "import random\n",
    "\n",
    "#Grid Search library\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "#Suppress futurewarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#Suppress convergence warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.simplefilter(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "# Setting Random State for replicability\n",
    "randomState = 777\n",
    "random.seed(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd9a18",
   "metadata": {},
   "source": [
    "#### Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce513607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Loading all data file separately \n",
    "vle = pd.read_csv('./anonymiseddata/vle.csv')\n",
    "assessments = pd.read_csv('./anonymiseddata/assessments.csv')\n",
    "courses = pd.read_csv('./anonymiseddata/courses.csv')\n",
    "studentAssessments = pd.read_csv('./anonymiseddata/studentAssessment.csv')\n",
    "studentInfo = pd.read_csv('./anonymiseddata/studentInfo.csv')\n",
    "studentRegistration = pd.read_csv('./anonymiseddata/studentRegistration.csv')\n",
    "studentVle = pd.read_csv('./anonymiseddata/studentVle.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946add74",
   "metadata": {},
   "source": [
    "# Business Understanding 1 #\n",
    "*Describe the purpose of the data set you selected (i.e., why was this data collected in the first place?). How will you measure the effectiveness of a good algorithm? Why does your chosen validation method make sense for this specific dataset and the stakeholders needs?*\n",
    "\n",
    "For Lab 3 we chose to use the \"Open University Learning Analytics Dataset\" (OULAD) which is an anonymized collection of information acquired from online students who were enrolled in the Open University (Milton Keynes, England) in 2013-2014. The OULAD was originally collected to better understand how students perform in an online-only environment by gathering information about their assessment (grade) results in addition to the frequency of usage with the online platforms. The data includes student demographic and registration information, as well as assessment results and interactions with the Virtual Learning Environment (VLE). There are multiple ways that the OULAD can be utilized, including prediction of grade results, better understanding of the factors that influence online student outcomes, or to compare online student results to in-person student results. The reason why this dataset is important is because it can be used by educators and universities to increase student success within an online learning environment. \n",
    "\n",
    "Our goal for Lab 3 was to explore the OULAD using K-Means clustering in order to find trends and insight into student learning styles. We chose K-Means clustering because it is an unsupervised algorithm that is straightforward to carry out, scalable, and can be used for large datasets. Our goal was to identify which features were associated with higher performance in terms of academic performance.\n",
    "\n",
    "We chose to evaluate the clustering model using the Silhouette Score, which calculates the distance between a data point and its assigned cluster. The Silhouette Score ranges from -1 to +1, and the closer the value is to +1 indicates that the means of the clusters are well-separated while a value closer to -1 indicates that the clusters are not distinct. We felt that the Silhouette Score was ideal because it provides a numeric value to the fitness of a cluster in addition to finding the optimal number of clusters for a given dataset.\n",
    "\n",
    "An additional tool we chose to use was Robust Scaler, an algorithm that scales the data to its interquartile range. Using the Robust Scaler was a way to standardize our data and remove any issues arising from outliers that could bias the outcome of the analysis.\n",
    "\n",
    "##### References\n",
    "[Kaggle: OULAD](https://www.kaggle.com/datasets/rocki37/open-university-learning-analytics-dataset)\n",
    "\n",
    "[sklearn.preprocessing.RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)\n",
    "\n",
    "[Silhouette Coefficient](https://towardsdatascience.com/silhouette-coefficient-validating-clustering-techniques-e976bb81d10c)\n",
    "\n",
    "[A Simple Explanation of K-Means Clustering](https://www.analyticsvidhya.com/blog/2020/10/a-simple-explanation-of-k-means-clustering/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d06c26",
   "metadata": {},
   "source": [
    "# Data Understanding 1 #\n",
    "*Describe the meaning and type of data (scale, values, etc.) for each attribute in the data file. Verify data quality: Are there missing values? Duplicate data? Outliers? Are those mistakes? How do you deal with these problems?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a069ac",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "\n",
    "The OULAD dataset is derived from a relational database schema, and contains seven CSV files that encompasses data collected from 32,593 students that were enrolled in 22 courses. There are a total of 43 variables and over 10 million instances. The CSV files and their contents are described below, information regarding each variable was taken from [HERE](https://analyse.kmi.open.ac.uk/open_dataset). It should be noted that \"modules\" refer to university courses and VLE refers to \"virtual learning environment\". Due to the nature of the data, there are duplicate entries as all 32,593 students took multiple courses\n",
    "\n",
    "#### Courses:\n",
    "* code_module **(categorical)** - code for the module, which serves as the identifier\n",
    "* code_presentation  **(categorical)**– code name of the presentation. It consists of the year and “B” for the presentation starting in February and “J” for the presentation starting in October.\n",
    "* length **(numeric)** - length of the module-presentation in days.\n",
    "\n",
    "#### Assessments\n",
    "* code_module **(categorical)** – identification code of the module, to which the assessment belongs.\n",
    "* code_presentation **(categorical)** - identification code of the presentation, to which the assessment belongs.\n",
    "* id_assessment **(numeric)** – identification number of the assessment.\n",
    "* assessment_type **(categorical)** – type of assessment. Three types of assessments exist: Tutor Marked Assessment (TMA), Computer Marked Assessment (CMA) and Final Exam (Exam).\n",
    "* date **(numeric)** – information about the final submission date of the assessment calculated as the number of days since the start of the module-presentation. The starting date of the presentation has number 0 (zero).\n",
    "* weight **(numeric)** - weight of the assessment in %. Typically, Exams are treated separately and have the weight 100%; the sum of all other assessments is 100%.\n",
    "\n",
    "#### VLE\n",
    "* id_site **(numeric)** – an identification number of the material.\n",
    "* code_module **(categorical)** – an identification code for module.\n",
    "* code_presentation **(categorical)** - the identification code of presentation.\n",
    "* activity_type **(categorical)** – the role associated with the module material.\n",
    "* week_from **(numeric)** – the week from which the material is planned to be used.\n",
    "* week_to **(numeric)** – week until which the material is planned to be used.;\n",
    "\n",
    "#### StudentInfo\n",
    "* code_module **(categorical)** – an identification code for a module on which the student is registered.\n",
    "* code_presentation **(categorical)** - the identification code of the presentation during which the student is registered on the module.\n",
    "* id_student **(numeric)** – a unique identification number for the student.\n",
    "* gender **(categorical)** – the student’s gender.\n",
    "* region **(categorical)** – identifies the geographic region, where the student lived while taking the module-presentation.\n",
    "* highest_education **(categorical)** – highest student education level on entry to the module presentation.\n",
    "* imd_band **(categorical)** – specifies the Index of Multiple Depravation band of the place where the student lived during the module-presentation.\n",
    "* age_band **(numeric)** – band of the student’s age.\n",
    "* num_of_prev_attempts **(numeric)** – the number times the student has attempted this module.\n",
    "* studied_credits **(numeric)** – the total number of credits for the modules the student is currently studying.\n",
    "* disability **(categorical)**  – indicates whether the student has declared a disability.\n",
    "* final_result **(categorical)** – student’s final result in the module-presentation.\n",
    "\n",
    "#### StudentRegistragion\n",
    "* code_module **(categorical)** – an identification code for a module.\n",
    "* code_presentation **(categorical)** - the identification code of the presentation.\n",
    "* id_student **(numeric)** – a unique identification number for the student.\n",
    "* date_registration **(numeric)** – the date of student’s registration on the module presentation, this is the number of days measured relative to the start of the module-presentation (e.g. the negative value -30 means that the student registered to module presentation 30 days before it started).\n",
    "* date_unregistration **(numeric)** – date of student unregistration from the module presentation, this is the number of days measured relative to the start of the module-presentation. Students, who completed the course have this field empty. Students who unregistered have Withdrawal as the value of the final_result column in the studentInfo.csv file.\n",
    "\n",
    "#### StudentAssessment\n",
    "* id_assessment **(numeric)** – the identification number of the assessment.\n",
    "* id_student **(numeric)** – a unique identification number for the student.\n",
    "* date_submitted **(numeric)** – the date of student submission, measured as the number of days since the start of the module presentation.\n",
    "* is_banked **(numeric)** – a status flag indicating that the assessment result has been transferred from a previous presentation.\n",
    "* score **(numeric)** – the student’s score in this assessment. The range is from 0 to 100. The score lower than 40 is interpreted as Fail. The marks are in the range from 0 to 100.\n",
    "\n",
    "#### StudentVLE\n",
    "* code_module **(categorical)** – an identification code for a module.\n",
    "* code_presentation **(categorical)** - the identification code of the module presentation.\n",
    "* id_student **(numeric)** – a unique identification number for the student.\n",
    "* id_site **(numeric)** - an identification number for the VLE material.\n",
    "* date **(numeric)** – the date of student’s interaction with the material measured as the number of days since the start of the module-presentation.\n",
    "* sum_click **(numeric)** – the number of times a student interacts with the material in that day.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a0e39c",
   "metadata": {},
   "source": [
    "The OULAD was originally used in a relational database schema with separated tables, hence the seven CSV files. In order to create a clustering model, we needed to merge these tables together into a singular dataframe. We found that using data from all of the 32,000 students was too intensive, and even sampling as few as 35 students resulted in 88,676 instances that took our machines hours to to calculate. Eventually, we settled on taking a random sample of 30 students for a total of 53,957 instances which more than covered the minimal number of transactions required for the project.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc52205b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 12)\n",
      "(53957, 28)\n",
      "Wall time: 1.88 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_module</th>\n",
       "      <th>code_presentation</th>\n",
       "      <th>id_student</th>\n",
       "      <th>gender</th>\n",
       "      <th>region</th>\n",
       "      <th>highest_education</th>\n",
       "      <th>imd_band</th>\n",
       "      <th>age_band</th>\n",
       "      <th>num_of_prev_attempts</th>\n",
       "      <th>studied_credits</th>\n",
       "      <th>...</th>\n",
       "      <th>score</th>\n",
       "      <th>assessment_type</th>\n",
       "      <th>date_y</th>\n",
       "      <th>weight</th>\n",
       "      <th>module_presentation_length</th>\n",
       "      <th>date_registration</th>\n",
       "      <th>date_unregistration</th>\n",
       "      <th>activity_type</th>\n",
       "      <th>week_from</th>\n",
       "      <th>week_to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCC</td>\n",
       "      <td>2014J</td>\n",
       "      <td>134190</td>\n",
       "      <td>F</td>\n",
       "      <td>Scotland</td>\n",
       "      <td>A Level or Equivalent</td>\n",
       "      <td>30-40%</td>\n",
       "      <td>0-35</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>...</td>\n",
       "      <td>78.0</td>\n",
       "      <td>CMA</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>269</td>\n",
       "      <td>-70.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>homepage</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCC</td>\n",
       "      <td>2014J</td>\n",
       "      <td>134190</td>\n",
       "      <td>F</td>\n",
       "      <td>Scotland</td>\n",
       "      <td>A Level or Equivalent</td>\n",
       "      <td>30-40%</td>\n",
       "      <td>0-35</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>...</td>\n",
       "      <td>78.0</td>\n",
       "      <td>CMA</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>269</td>\n",
       "      <td>-70.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>resource</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCC</td>\n",
       "      <td>2014J</td>\n",
       "      <td>134190</td>\n",
       "      <td>F</td>\n",
       "      <td>Scotland</td>\n",
       "      <td>A Level or Equivalent</td>\n",
       "      <td>30-40%</td>\n",
       "      <td>0-35</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>...</td>\n",
       "      <td>78.0</td>\n",
       "      <td>CMA</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>269</td>\n",
       "      <td>-70.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>forumng</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCC</td>\n",
       "      <td>2014J</td>\n",
       "      <td>134190</td>\n",
       "      <td>F</td>\n",
       "      <td>Scotland</td>\n",
       "      <td>A Level or Equivalent</td>\n",
       "      <td>30-40%</td>\n",
       "      <td>0-35</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>...</td>\n",
       "      <td>78.0</td>\n",
       "      <td>CMA</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>269</td>\n",
       "      <td>-70.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>forumng</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCC</td>\n",
       "      <td>2014J</td>\n",
       "      <td>134190</td>\n",
       "      <td>F</td>\n",
       "      <td>Scotland</td>\n",
       "      <td>A Level or Equivalent</td>\n",
       "      <td>30-40%</td>\n",
       "      <td>0-35</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>...</td>\n",
       "      <td>78.0</td>\n",
       "      <td>CMA</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>269</td>\n",
       "      <td>-70.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>homepage</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  code_module code_presentation  id_student gender    region  \\\n",
       "0         CCC             2014J      134190      F  Scotland   \n",
       "1         CCC             2014J      134190      F  Scotland   \n",
       "2         CCC             2014J      134190      F  Scotland   \n",
       "3         CCC             2014J      134190      F  Scotland   \n",
       "4         CCC             2014J      134190      F  Scotland   \n",
       "\n",
       "       highest_education imd_band age_band  num_of_prev_attempts  \\\n",
       "0  A Level or Equivalent   30-40%     0-35                     0   \n",
       "1  A Level or Equivalent   30-40%     0-35                     0   \n",
       "2  A Level or Equivalent   30-40%     0-35                     0   \n",
       "3  A Level or Equivalent   30-40%     0-35                     0   \n",
       "4  A Level or Equivalent   30-40%     0-35                     0   \n",
       "\n",
       "   studied_credits  ... score assessment_type  date_y  weight  \\\n",
       "0              120  ...  78.0             CMA    18.0     2.0   \n",
       "1              120  ...  78.0             CMA    18.0     2.0   \n",
       "2              120  ...  78.0             CMA    18.0     2.0   \n",
       "3              120  ...  78.0             CMA    18.0     2.0   \n",
       "4              120  ...  78.0             CMA    18.0     2.0   \n",
       "\n",
       "   module_presentation_length  date_registration  date_unregistration  \\\n",
       "0                         269              -70.0                 32.0   \n",
       "1                         269              -70.0                 32.0   \n",
       "2                         269              -70.0                 32.0   \n",
       "3                         269              -70.0                 32.0   \n",
       "4                         269              -70.0                 32.0   \n",
       "\n",
       "   activity_type  week_from week_to  \n",
       "0       homepage        NaN     NaN  \n",
       "1       resource        NaN     NaN  \n",
       "2        forumng        NaN     NaN  \n",
       "3        forumng        NaN     NaN  \n",
       "4       homepage        NaN     NaN  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Randomly sampling 40 students to generate dataset\n",
    "studentSample = studentInfo.sample(n=30, random_state = randomState)\n",
    "print(studentSample.shape)\n",
    "\n",
    "# vle\n",
    "# assessments X\n",
    "# courses X\n",
    "# studentAssessments X\n",
    "# studentInfo X\n",
    "# studentRegistration X\n",
    "# studentVleSample X\n",
    "\n",
    "# compiling dataframe from data files\n",
    "df = pd.merge(studentSample, studentVle, on=['code_module', 'code_presentation', 'id_student'])\n",
    "df2 = pd.merge(studentAssessments, assessments, on='id_assessment')\n",
    "df = pd.merge(df, df2, on=['code_module', 'code_presentation', 'id_student'], how='left')\n",
    "df = pd.merge(df, courses, on= ['code_module', 'code_presentation'], how='left')\n",
    "df = pd.merge(df, studentRegistration, on=['code_module', 'code_presentation', 'id_student'], how='left')\n",
    "df = pd.merge(df, vle, on=['id_site', 'code_module', 'code_presentation'], how='left')\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ec82e",
   "metadata": {},
   "source": [
    "### Duplicates\n",
    "\n",
    "We checked the dataset for duplicate entries and found that there were 4,593 duplicates. We chose to remove these duplicate entries before moving on to check missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15c65804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4593"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eb7b593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(keep = 'first', inplace = True)\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9d3839",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7b2ee",
   "metadata": {},
   "source": [
    "From the cell below, we saw that date_unregistration had null entries in over 96% of the data, and both week_from and week_to had null entries for over 86% of the data. Since these columns seemed to be unnecessary, we chose to drop all three as they consisted mostly of missing values. \n",
    "\n",
    "Running the same code on the dataset after dropping the three columns showed that the percentages of missing values in the remaining columns were zero except for \"date_y\" which had less than 3% missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16d385d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: \n",
      " code_module                    0.000000\n",
      "code_presentation              0.000000\n",
      "id_student                     0.000000\n",
      "gender                         0.000000\n",
      "region                         0.000000\n",
      "highest_education              0.000000\n",
      "imd_band                       0.000000\n",
      "age_band                       0.000000\n",
      "num_of_prev_attempts           0.000000\n",
      "studied_credits                0.000000\n",
      "disability                     0.000000\n",
      "final_result                   0.000000\n",
      "id_site                        0.000000\n",
      "date_x                         0.000000\n",
      "sum_click                      0.000000\n",
      "id_assessment                  0.000000\n",
      "date_submitted                 0.000000\n",
      "is_banked                      0.000000\n",
      "score                          0.000000\n",
      "assessment_type                0.000000\n",
      "date_y                         2.937363\n",
      "weight                         0.000000\n",
      "module_presentation_length     0.000000\n",
      "date_registration              0.000000\n",
      "date_unregistration           87.286282\n",
      "activity_type                  0.000000\n",
      "week_from                     89.387003\n",
      "week_to                       89.387003\n",
      "dtype: float64 \n",
      "\n",
      "\n",
      "Missing values after dopping columns: \n",
      " code_module                   0.000000\n",
      "code_presentation             0.000000\n",
      "id_student                    0.000000\n",
      "gender                        0.000000\n",
      "region                        0.000000\n",
      "highest_education             0.000000\n",
      "imd_band                      0.000000\n",
      "age_band                      0.000000\n",
      "num_of_prev_attempts          0.000000\n",
      "studied_credits               0.000000\n",
      "disability                    0.000000\n",
      "final_result                  0.000000\n",
      "id_site                       0.000000\n",
      "date_x                        0.000000\n",
      "sum_click                     0.000000\n",
      "id_assessment                 0.000000\n",
      "date_submitted                0.000000\n",
      "is_banked                     0.000000\n",
      "score                         0.000000\n",
      "assessment_type               0.000000\n",
      "date_y                        2.937363\n",
      "weight                        0.000000\n",
      "module_presentation_length    0.000000\n",
      "date_registration             0.000000\n",
      "activity_type                 0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Missing values: \\n', df.isnull().sum() * 100 / len(df),'\\n\\n')\n",
    "missingValues = df[df.isnull()]\n",
    "try:\n",
    "    df = df.drop(columns = ['week_from', 'week_to', 'date_unregistration'])\n",
    "except:\n",
    "    print('These columns have already been dropped\\n\\n')\n",
    "print('Missing values after dopping columns: \\n', df.isnull().sum() * 100 / len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07718173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47914, 20)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "try:\n",
    "    df.drop(columns=['id_student','id_assessment','id_site','code_module','code_presentation'],inplace=True)\n",
    "except:\n",
    "    print(\"These columns have already been dropped\")\n",
    "    \n",
    "#converting variables to their correct type\n",
    "df.date_registration = df['date_registration'].astype(int)\n",
    "df.date_y = df['date_y'].astype(int)\n",
    "df.score = df['score'].astype(int)\n",
    "df.num_of_prev_attempts = df['num_of_prev_attempts'].astype(object)\n",
    "df.studied_credits = df['studied_credits'].astype(object)\n",
    "df.is_banked = df['is_banked'].astype(object)\n",
    "df.module_presentation_length = df['module_presentation_length'].astype(object)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597d6d69",
   "metadata": {},
   "source": [
    "We converted Date_registration, date_y, and score to integer values since there were no counts of values with decimals. We also converted num_of_prev_attempts, studied_credits, is_banked, and module_presentation_length to objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1deef1",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f313fa14",
   "metadata": {},
   "source": [
    "As shown in the code output below, many of the variables in the OULAD were categorical making it necessary to One Hot Encode the data in order to analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b008c96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender                         object\n",
       "region                         object\n",
       "highest_education              object\n",
       "imd_band                       object\n",
       "age_band                       object\n",
       "num_of_prev_attempts           object\n",
       "studied_credits                object\n",
       "disability                     object\n",
       "final_result                   object\n",
       "date_x                          int64\n",
       "sum_click                       int64\n",
       "date_submitted                  int64\n",
       "is_banked                      object\n",
       "score                           int32\n",
       "assessment_type                object\n",
       "date_y                          int32\n",
       "weight                        float64\n",
       "module_presentation_length     object\n",
       "date_registration               int32\n",
       "activity_type                  object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "694f922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating variables for indexing continuous and categorical variables\n",
    "conCol = ['date_x', 'sum_click', 'date_submitted', 'score', 'date_y', 'weight', 'date_registration']\n",
    "\n",
    "catCol = ['gender', 'region', 'highest_education', 'is_banked',  'num_of_prev_attempts', 'studied_credits',\n",
    "          'imd_band', 'age_band', 'disability', 'final_result', 'assessment_type', 'module_presentation_length', 'activity_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dc7bfb",
   "metadata": {},
   "source": [
    "The following cell scales the data using a standard scaling technique and one hot encodes the dataset including outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a73c4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 83 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_x</th>\n",
       "      <th>sum_click</th>\n",
       "      <th>date_submitted</th>\n",
       "      <th>score</th>\n",
       "      <th>date_y</th>\n",
       "      <th>weight</th>\n",
       "      <th>date_registration</th>\n",
       "      <th>gender_F</th>\n",
       "      <th>gender_M</th>\n",
       "      <th>region_East Anglian Region</th>\n",
       "      <th>...</th>\n",
       "      <th>activity_type_oucontent</th>\n",
       "      <th>activity_type_ouelluminate</th>\n",
       "      <th>activity_type_ouwiki</th>\n",
       "      <th>activity_type_page</th>\n",
       "      <th>activity_type_questionnaire</th>\n",
       "      <th>activity_type_quiz</th>\n",
       "      <th>activity_type_resource</th>\n",
       "      <th>activity_type_sharedsubpage</th>\n",
       "      <th>activity_type_subpage</th>\n",
       "      <th>activity_type_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.597173</td>\n",
       "      <td>-0.417423</td>\n",
       "      <td>-1.41937</td>\n",
       "      <td>-0.042726</td>\n",
       "      <td>-1.451877</td>\n",
       "      <td>-0.745647</td>\n",
       "      <td>-0.569523</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.597173</td>\n",
       "      <td>-0.417423</td>\n",
       "      <td>-1.41937</td>\n",
       "      <td>-0.042726</td>\n",
       "      <td>-1.451877</td>\n",
       "      <td>-0.745647</td>\n",
       "      <td>-0.569523</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.359935</td>\n",
       "      <td>-0.417423</td>\n",
       "      <td>-1.41937</td>\n",
       "      <td>-0.042726</td>\n",
       "      <td>-1.451877</td>\n",
       "      <td>-0.745647</td>\n",
       "      <td>-0.569523</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.359935</td>\n",
       "      <td>-0.243497</td>\n",
       "      <td>-1.41937</td>\n",
       "      <td>-0.042726</td>\n",
       "      <td>-1.451877</td>\n",
       "      <td>-0.745647</td>\n",
       "      <td>-0.569523</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.359935</td>\n",
       "      <td>0.452205</td>\n",
       "      <td>-1.41937</td>\n",
       "      <td>-0.042726</td>\n",
       "      <td>-1.451877</td>\n",
       "      <td>-0.745647</td>\n",
       "      <td>-0.569523</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     date_x  sum_click  date_submitted     score    date_y    weight  \\\n",
       "0 -1.597173  -0.417423        -1.41937 -0.042726 -1.451877 -0.745647   \n",
       "1 -1.597173  -0.417423        -1.41937 -0.042726 -1.451877 -0.745647   \n",
       "2 -1.359935  -0.417423        -1.41937 -0.042726 -1.451877 -0.745647   \n",
       "3 -1.359935  -0.243497        -1.41937 -0.042726 -1.451877 -0.745647   \n",
       "4 -1.359935   0.452205        -1.41937 -0.042726 -1.451877 -0.745647   \n",
       "\n",
       "   date_registration  gender_F  gender_M  region_East Anglian Region  ...  \\\n",
       "0          -0.569523         1         0                           0  ...   \n",
       "1          -0.569523         1         0                           0  ...   \n",
       "2          -0.569523         1         0                           0  ...   \n",
       "3          -0.569523         1         0                           0  ...   \n",
       "4          -0.569523         1         0                           0  ...   \n",
       "\n",
       "   activity_type_oucontent  activity_type_ouelluminate  activity_type_ouwiki  \\\n",
       "0                        0                           0                     0   \n",
       "1                        0                           0                     0   \n",
       "2                        0                           0                     0   \n",
       "3                        0                           0                     0   \n",
       "4                        0                           0                     0   \n",
       "\n",
       "   activity_type_page  activity_type_questionnaire  activity_type_quiz  \\\n",
       "0                   0                            0                   0   \n",
       "1                   0                            0                   0   \n",
       "2                   0                            0                   0   \n",
       "3                   0                            0                   0   \n",
       "4                   0                            0                   0   \n",
       "\n",
       "   activity_type_resource  activity_type_sharedsubpage  activity_type_subpage  \\\n",
       "0                       0                            0                      0   \n",
       "1                       1                            0                      0   \n",
       "2                       0                            0                      0   \n",
       "3                       0                            0                      0   \n",
       "4                       0                            0                      0   \n",
       "\n",
       "   activity_type_url  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  \n",
       "\n",
       "[5 rows x 76 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Declaring Scalar object\n",
    "scl_obj = StandardScaler()\n",
    "#scl_obj = RobustScaler()\n",
    "\n",
    "#One hot encoding of ALL categorical variables\n",
    "OneHotDF = pd.get_dummies(df[catCol],drop_first=False)\n",
    "\n",
    "#Scaling non-encoded data\n",
    "conVar = df.select_dtypes(exclude='object')\n",
    "colnames = pd.Series(conVar.columns)\n",
    "conVarScaled = scl_obj.fit_transform(conVar)\n",
    "conVarScaled = pd.DataFrame(data = conVarScaled, columns= colnames)\n",
    "\n",
    "#Combining with continuous variables from cleaned dataset\n",
    "OneHotDF = OneHotDF.reset_index()\n",
    "OneHotDF.pop('index')\n",
    "OneHotDF = pd.concat([conVarScaled,OneHotDF], axis = 1)\n",
    "OneHotDF.head()\n",
    "\n",
    "#https://github.com/jakemdrew/DataMiningNotebooks/blob/master/01.%20Pandas.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7a3057",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "Even though we planned to investigate the use of a Robust Scaler, since the OULAD was entirely numeric we could look through the data for outliers. The code below was used to calculate the 25th and 75th quantiles from the data and then report the columns that had values outside of the interquantile range (IQR). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7fcb568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q3</th>\n",
       "      <th>IQR</th>\n",
       "      <th>lowerGate</th>\n",
       "      <th>upperGate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variable Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>date_y</th>\n",
       "      <td>-0.884680</td>\n",
       "      <td>1.041152</td>\n",
       "      <td>1.925832</td>\n",
       "      <td>-3.773428</td>\n",
       "      <td>3.929900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_submitted</th>\n",
       "      <td>-0.992433</td>\n",
       "      <td>0.791555</td>\n",
       "      <td>1.783988</td>\n",
       "      <td>-3.668415</td>\n",
       "      <td>3.467537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_x</th>\n",
       "      <td>-0.924998</td>\n",
       "      <td>0.788389</td>\n",
       "      <td>1.713387</td>\n",
       "      <td>-3.495078</td>\n",
       "      <td>3.358469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_registration</th>\n",
       "      <td>-0.777378</td>\n",
       "      <td>0.781530</td>\n",
       "      <td>1.558908</td>\n",
       "      <td>-3.115740</td>\n",
       "      <td>3.119892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>-0.809064</td>\n",
       "      <td>0.522701</td>\n",
       "      <td>1.331766</td>\n",
       "      <td>-2.806713</td>\n",
       "      <td>2.520350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>-0.472970</td>\n",
       "      <td>0.817762</td>\n",
       "      <td>1.290732</td>\n",
       "      <td>-2.409067</td>\n",
       "      <td>2.753860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum_click</th>\n",
       "      <td>-0.417423</td>\n",
       "      <td>-0.069572</td>\n",
       "      <td>0.347851</td>\n",
       "      <td>-0.939200</td>\n",
       "      <td>0.452205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Q1        Q3       IQR  lowerGate  upperGate\n",
       "Variable Name                                                        \n",
       "date_y            -0.884680  1.041152  1.925832  -3.773428   3.929900\n",
       "date_submitted    -0.992433  0.791555  1.783988  -3.668415   3.467537\n",
       "date_x            -0.924998  0.788389  1.713387  -3.495078   3.358469\n",
       "date_registration -0.777378  0.781530  1.558908  -3.115740   3.119892\n",
       "weight            -0.809064  0.522701  1.331766  -2.806713   2.520350\n",
       "score             -0.472970  0.817762  1.290732  -2.409067   2.753860\n",
       "sum_click         -0.417423 -0.069572  0.347851  -0.939200   0.452205"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1 = OneHotDF[conCol].quantile(0.25)\n",
    "Q3 = OneHotDF[conCol].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "# print(IQR)\n",
    "\n",
    "IQRDF = pd.DataFrame(data = {'Q1':Q1, 'Q3':Q3, 'IQR':IQR})\n",
    "IQRDF.index.name = 'Variable Name'\n",
    "IQRDF['lowerGate'] = Q1-1.5*IQR\n",
    "IQRDF['upperGate'] = Q3+1.5*IQR\n",
    "IQRDF.sort_values(by='IQR', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5187bbcc",
   "metadata": {},
   "source": [
    "The code below was used to check for rows that were outside the range of the lower and upper bounds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf3632dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date_x     sum_click  date_submitted         score        date_y  \\\n",
      "count  47914.000000  47914.000000    47914.000000  47914.000000  47914.000000   \n",
      "mean     105.182473      3.400008      114.087114     78.794444    128.068832   \n",
      "std       75.873907      5.749644       65.584087     18.594296     75.812181   \n",
      "min      -25.000000      1.000000       10.000000     10.000000     12.000000   \n",
      "25%       35.000000      1.000000       49.000000     70.000000     61.000000   \n",
      "50%      107.000000      2.000000      119.000000     80.000000    124.000000   \n",
      "75%      165.000000      3.000000      166.000000     94.000000    207.000000   \n",
      "max      268.000000     98.000000      241.000000    100.000000    261.000000   \n",
      "\n",
      "             weight  date_registration  \n",
      "count  47914.000000       47914.000000  \n",
      "mean      13.757764         -48.079914  \n",
      "std       15.768704          38.488883  \n",
      "min        0.000000        -165.000000  \n",
      "25%        1.000000         -78.000000  \n",
      "50%       10.000000         -29.000000  \n",
      "75%       22.000000         -18.000000  \n",
      "max      100.000000           3.000000  \n",
      "There are outliers for the date_x variable.\n",
      "There are outliers for the sum_click variable.\n",
      "There are outliers for the date_submitted variable.\n",
      "There are outliers for the score variable.\n",
      "There are outliers for the date_y variable.\n",
      "There are outliers for the weight variable.\n",
      "There are outliers for the date_registration variable.\n"
     ]
    }
   ],
   "source": [
    "print(df[conCol].describe())\n",
    "\n",
    "for i in range(0,len(IQRDF.index)):\n",
    "    lowerCount = len((OneHotDF[OneHotDF[IQRDF.index[i]] < IQRDF.lowerGate[i]]))\n",
    "    upperCount = len((OneHotDF[OneHotDF[IQRDF.index[i]] > IQRDF.upperGate[i]]))\n",
    "\n",
    "    if ((lowerCount !=0) & (upperCount !=0)):\n",
    "        print('The number of outliers outside the lower and upper gates for the '+ \n",
    "              IQRDF.index[i] + ' variable is', lowerCount + upperCount)\n",
    "    else:\n",
    "        print('There are outliers for the ' + IQRDF.index[i] + ' variable.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49722fa",
   "metadata": {},
   "source": [
    "According to the results printed above, there were outliers within the dataset. In the code below, we created a new dataset with the outliers removed so that we could compare results from a dataset with outliers removed to results from a dataset with outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a4eff34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before removing outliers: (47914, 76)\n",
      "shape after removing outliers: (39778, 76)\n"
     ]
    }
   ],
   "source": [
    "outliersRemoved = OneHotDF\n",
    "print('shape before removing outliers:',outliersRemoved.shape)\n",
    "outliersRemoved = outliersRemoved[~((outliersRemoved < (Q1 - 1.5 * IQR)) |(outliersRemoved > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "print('shape after removing outliers:',outliersRemoved.shape)\n",
    "\n",
    "# # Performing robust scaling on dataset with outliers removed\n",
    "# sc = RobustScaler()\n",
    "# outliersRemovedTransformed = sc.fit_transform(outliersRemoved)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db27c14e",
   "metadata": {},
   "source": [
    "The following cell creates a one hot encoded dataframe that includes outliers but uses robust scaling to approach the outliers to avoid dropping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc5cab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declaring Scalar object using Robust Scaling\n",
    "scl_obj = RobustScaler()\n",
    "\n",
    "#One hot encoding of ALL categorical variables\n",
    "OneHotDF2 = pd.get_dummies(df[catCol],drop_first=False)\n",
    "\n",
    "#Scaling non-encoded data\n",
    "conVar = df.select_dtypes(exclude='object')\n",
    "colnames = pd.Series(conVar.columns)\n",
    "conVarScaled = scl_obj.fit_transform(conVar)\n",
    "conVarScaled = pd.DataFrame(data = conVarScaled, columns= colnames)\n",
    "\n",
    "#Combining with continuous variables from cleaned dataset\n",
    "OneHotDF2 = OneHotDF2.reset_index()\n",
    "OneHotDF2.pop('index')\n",
    "outliersRobust = pd.concat([conVarScaled,OneHotDF2], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43323054",
   "metadata": {},
   "source": [
    "### Principal Component Analysis to reduce dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aecd3602",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8604\\115108630.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpcadf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneHotDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msegmentation_std\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpcadf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpcadf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scaler' is not defined"
     ]
    }
   ],
   "source": [
    "#pca + kmeans attempt\n",
    "from sklearn.decomposition import PCA\n",
    "pcadf = OneHotDF\n",
    "segmentation_std = scaler.fit_transform(pcadf)\n",
    "pca = PCA()\n",
    "pca.fit(pcadf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0792a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8005a112",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8))\n",
    "plt.plot(range(1,77), pca.explained_variance_ratio_.cumsum(), marker = 'o', linestyle = '--')\n",
    "plt.title(\"Variance Explained by Principal Components\")\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd66aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(pcadf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79e89e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_pca = pca.transform(pcadf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edba66b",
   "metadata": {},
   "source": [
    "# Data Understanding 2 #\n",
    "*Visualize the any important attributes appropriately. Important: Provide an interpretation for any charts or graphs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a388c0",
   "metadata": {},
   "source": [
    "We wanted to check the relationships of some of the variables, so we created two functions in the cell below to analyze the data. We used the dataset with no outlier treatment. It should be continously noted that the sample used for the following figures was severely reduced and it is  unlikely that our sample is sufficient to reflect the actual distributions of the original dataset.\n",
    "\n",
    "The first function creates a stacked boxplot of a categorical variable compared to \"Final_Result\" (AKA: final grade at the end of the course) as well as outputs the counts of each categorical level per final_result level.\n",
    "\n",
    "The second function was used to create a series of box plots and histograms for the continuous variables, this was done so that we could check for outliers and distributions. A triangle symbol in the box plots indicates the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeeef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_plot(x):\n",
    "    sns.set(palette='nipy_spectral')\n",
    "    tab1 = pd.crosstab(x,df['final_result'],margins=True)\n",
    "    print(tab1)\n",
    "    print('-'*100)\n",
    "    tab = pd.crosstab(x,df['final_result'],normalize='index')\n",
    "    tab.plot(kind='bar',stacked=True,figsize=(15,5))\n",
    "    #plt.legend(loc='lower left', frameon=False)\n",
    "    #plt.legend(loc=\"upper left\", bbox_to_anchor=(0,1))\n",
    "    plt.legend(loc='upper right',bbox_to_anchor=(1.1, 1))\n",
    "    plt.show()\n",
    "    \n",
    "def histogram_boxplot(feature, figsize=(15, 10), bins=None):\n",
    "    \"\"\"Boxplot and histogram combined\n",
    "    feature: 1-d feature array\n",
    "    figsize: size of fig (default (9,8))\n",
    "    bins: number of bins (default None / auto)\n",
    "    \"\"\"\n",
    "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
    "        nrows=2,  # Number of rows of the subplot grid= 2\n",
    "        sharex=True,  # x-axis will be shared among all subplots\n",
    "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
    "        figsize=figsize,\n",
    "    )  # creating the 2 subplots\n",
    "    sns.boxplot(\n",
    "        feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
    "    )  # boxplot will be created and a star will indicate the mean value of the column\n",
    "    sns.distplot(\n",
    "        feature, kde=F, ax=ax_hist2, bins=bins, color=\"orange\"\n",
    "    ) if bins else sns.distplot(\n",
    "        feature, kde=False, ax=ax_hist2, color=\"tab:cyan\"\n",
    "    )  # For histogram\n",
    "    ax_hist2.axvline(\n",
    "        np.mean(feature), color=\"purple\", linestyle=\"--\"\n",
    "    )  # Add mean to the histogram\n",
    "    ax_hist2.axvline(\n",
    "        np.median(feature), color=\"black\", linestyle=\"-\"\n",
    "    )  # Add median to the histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df9e316",
   "metadata": {},
   "source": [
    "The first variable we looked at was \"gender\", we found that the distribution of between males and females for each level of final_result varied, with no \"Distinction\" results occurring in the Female level and significantly more \"Fail\" results when compared to the Male level. However, it should be noted that the random sample taken from the complete dataset appeared to be skewed towards male students with a total of 38,595 vs 13,780 for female students. It is possible that increasing the size of the random sample of students would result in a more equal distribution of grade results and gender. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22063f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_plot(df[\"gender\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b961083",
   "metadata": {},
   "source": [
    "The next categorical variable we examined was \"Region\". It was notable that all grade results for the London Region were \"Fail\" and the South West Region consisted entirely of \"Withdrawn\" results. The East Anglian and Yorkshire region showed results only for \"Fail\" and \"Pass\", with \"Fail\" taking the majority in both regions. The North Western and South East regions had the only students to receive \"Distinction\" grades while all students in the South West region had \"Withdrawn\" results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b5d9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_plot(df[\"region\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ec4514",
   "metadata": {},
   "source": [
    "The results for \"imd_band\" (Indices of Multiple Deprivation - a measure of poverty where lower percentages indicate increased poverty) indicated that all students in the 0-10% and 90-100% bands passed their courses. The 10-20% band was almost equally distributed between \"Fail\" and \"Withdrawn\" results, and the 50-60% band consisted entirely of \"Fail\" results. The 70-80% IMD band had the most believable distribution, with results representing all four levels of the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd17f330",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_plot(df[\"imd_band\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9f97ad",
   "metadata": {},
   "source": [
    "For the \"activity_type\" variable, we noted that those students who only referenced the \"shared sub page\" had entirely withdrawn results while those students \"dataplus\", \"folder\", \"glossary\", and \"questionairre\" pages had entirely \"pass\" results. One other notable variable was the \"ouelluminate\" column, which showed entirely \"fail\" results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4317c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_plot(df[\"activity_type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3bf394",
   "metadata": {},
   "source": [
    "When comparing ages of students vs final_result, we found that there were no \"Distinction\" results in the 35-55 age band. There were significantly more \"Fail\" and \"Withdrawn\" results for the 35-55 age band (3,402 and 5,426) vs the 0-35 age band (1,791 and 1,554).\n",
    "\n",
    "These results are logical in the sense that the age band at 0-35 would include younger students who are starting a career by pursuing a degree, while students from 35-55 could possibly just be taking furthering education courses or, in the case of already being in a career, limited in the amount of time they can devote to a course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c644e9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_plot(df[\"age_band\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903a2e83",
   "metadata": {},
   "source": [
    "For the assessment types, the levels were CMA (Computer Marked Assessment), TMA (Tutor Marked Assessment), and traditional Exam.\n",
    "\n",
    "Interestingly, there were no Fail, Withdrawn, or Distinction results for the Exam level. For the TMA and CMA levels, the percentage of Pass results was the same for both types at 58%, which was similar to the outcome of Fail results at 17% and 21%, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd883e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_plot(df[\"assessment_type\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9be0868",
   "metadata": {},
   "source": [
    "For the continous variables, we chose to use box plots and histograms. The commented-out code below produces a loop that we used to review all of the continous variables, but we chose to focus on those variables that we had found outliers in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3613fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in conCol:\n",
    "    #histogram_boxplot(ohdf[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84281e7b",
   "metadata": {},
   "source": [
    "The distribution of the \"sum_click\" (the number of times a student interacts with the online material in a day) showed several outliers from students who accessed their online courses more frequently, so much so that the mean value was pulled outside of the box, and a right skewed distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edc1853",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(df['sum_click'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef81d3ba",
   "metadata": {},
   "source": [
    "The output for \"Score\" (the actual assessment score) showed seven outliers on the left and a left skewed distribution. This indicated that there were only outliers from students who had performed very poorly on their assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7384e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(df['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c13be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(df['date_registration'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5418e2e",
   "metadata": {},
   "source": [
    "The box plot for \"Weight\" (calculated weights of the assessments) showed a single outlier, which likely indicated a course whose entire grade was based on a single assessment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a0c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(df['weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b9d5c",
   "metadata": {},
   "source": [
    "The box plot for \"Date_Registration\" (the number of days relative to the start of a course that a student registers for the course) indicated that the majority of students registered a few days before the course began,and no outliers were found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bca5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(df['date_registration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f09732",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "outliersRobust[['final_result_Pass','score','num_of_prev_attempts_0','num_of_prev_attempts_1','num_of_prev_attempts_2', ]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c694dbac",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 1 #\n",
    "*Train and adjust parameters*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df4d855",
   "metadata": {},
   "source": [
    "After randomly sampling the original OULAD, removing missing values, and One Hot Encoding the resulting dataset, we began the cluster analysis. We started by creating a K-Means clustering model using the **Number of Previous Attempts** and the **Final Result = Pass** variables. After some trial and error, we settled on n=4 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc38e0d3",
   "metadata": {},
   "source": [
    "### K Means on Dataset with Robust Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5e6d47",
   "metadata": {},
   "source": [
    "the following cell calculates distortion and k through elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feca6ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clusters=range(1,30,1)\n",
    "meanDistortions=[]\n",
    "for k in clusters:\n",
    "    model=KMeans(n_clusters=k, random_state = randomState)\n",
    "    model.fit(outliersRobust)\n",
    "    prediction=model.predict(outliersRobust)\n",
    "    distortion=sum(np.min(cdist(outliersRobust, model.cluster_centers_, 'euclidean'),axis=1)) / outliersRobust.shape[0]\n",
    "    meanDistortions.append(distortion)\n",
    "    print('Number of Clusters:', k, '\\tAverage Distortion:',distortion)\n",
    "    \n",
    "plt.plot(clusters, meanDistortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Average distortion')\n",
    "plt.title('Selecting k with the Elbow Method')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba0ea3e",
   "metadata": {},
   "source": [
    "The following cell compares k values using silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c568186",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# K Means Goodness loop\n",
    "nclust = 15\n",
    "cluster_goodness = []\n",
    "\n",
    "for n in range(3,nclust,1):\n",
    "    model = KMeans(n_clusters=n,random_state=randomState, init='k-means++')\n",
    "    model.fit(outliersRobust)\n",
    "    clusterLabels = model.labels_\n",
    "    goodness = silhouette_score(X=outliersRobust, labels=clusterLabels)\n",
    "    cluster_goodness.append([(n),goodness])\n",
    "    print(\"K = {0}, Goodness = {1}\".format(n, goodness))\n",
    "    \n",
    "cluster_goodness = pd.DataFrame.from_records(data = cluster_goodness,\n",
    "                                            columns = [\"n\", \"goodness\"])\n",
    "\n",
    "print('\\nThe max goodness is',cluster_goodness.goodness.max(),\n",
    "      'at K =',cluster_goodness.n[cluster_goodness.goodness.idxmax()])\n",
    "\n",
    "plt.plot(cluster_goodness.goodness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f8203",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Finding optimal no. of clusters with silhouette coefficients\n",
    "visualizer = SilhouetteVisualizer(KMeans(3, random_state = randomState))\n",
    "visualizer.fit(outliersRobust)    \n",
    "visualizer.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70a87af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Finding optimal no. of clusters with silhouette coefficients\n",
    "visualizer = SilhouetteVisualizer(KMeans(4, random_state = randomState))\n",
    "visualizer.fit(outliersRobust)    \n",
    "visualizer.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0373fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Finding optimal no. of clusters with silhouette coefficients\n",
    "visualizer = SilhouetteVisualizer(KMeans(5, random_state = randomState))\n",
    "visualizer.fit(outliersRobust)    \n",
    "visualizer.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93d1c38",
   "metadata": {},
   "source": [
    "### K Modes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b85ab",
   "metadata": {},
   "source": [
    "Attempt at kmodes\n",
    "\n",
    "K Modes algorithm clusters based on categorical data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402e34e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "kmodeDF = df\n",
    "\n",
    "# Building the model with 3 clusters\n",
    "kmode = KModes(n_clusters=5, random_state = randomState, verbose = 1, n_init = 5, init='Huang')\n",
    "clusters = kmode.fit_predict(df[catCol])\n",
    "kmodeDF['cluster'] = clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b105aa",
   "metadata": {},
   "source": [
    "### PCA with K-Means Clustering\n",
    "\n",
    "Since our dataset was so large, we decided to reduce its dimensionality by using Principal Component Analysis and then apply K-means clustering to the resulting PCs.\n",
    "\n",
    "\n",
    "[Tutorial on Combining PCA & K-Means Clustering](https://365datascience.com/tutorials/python-tutorials/pca-k-means/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646d9019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the PCA-transformed data for kmeans\n",
    "wcss = []\n",
    "for i in range(1,27):\n",
    "    kmeans_pca = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
    "    kmeans_pca.fit(scores_pca)\n",
    "    wcss.append(kmeans_pca.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d118a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8))\n",
    "plt.plot(range(1,27), wcss, marker = 'o', linestyle = '--')\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"WCSS\")\n",
    "plt.title(\"K-Means with PCA Clustering\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04ae863",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pca = KMeans(n_clusters = 7, init = 'k-means++', random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e403f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pca.fit(scores_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ff4d53",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 2\t#\n",
    "*Evaluate and Compare*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33201c74",
   "metadata": {},
   "source": [
    "In the following cells we applied the Robust Scaler to the dataset and used the Silhouette Score to calculate Goodness in an attempt to find the optimal number of clusters. We chose to loop through 49 clusters in order to find the optimal cluster and print a figure showing the Goodness score vs cluster. \n",
    "\n",
    "The results of the analysis showed that the best cluster in terms of goodness was k=27 which produced a goodness value 0.3885.\n",
    "\n",
    "[Data Visualization in Python Talk](https://github.com/y2ee201/Data-Visualization-in-Python-Talk/blob/master/OULAD%20Student%20Segmentation%20.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f96566",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 3 #\n",
    "*Visualize Results*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f81afb2",
   "metadata": {},
   "source": [
    "Im trying to mess around with visualizing our cluster results: \n",
    "\n",
    "https://stackoverflow.com/questions/46844654/how-to-visualize-kmeans-clustering-on-multidimensional-data\n",
    "\n",
    "https://stats.stackexchange.com/questions/52625/visually-plotting-multi-dimensional-cluster-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471d97e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Set Figure Size\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "finalCM = KMeans(n_clusters=4,random_state=randomState, init='k-means++')\n",
    "finalCM.fit(outliersRobust)\n",
    "predict = finalCM.predict(outliersRobust)\n",
    "outliersRobust['cluster'] = predict\n",
    "pd.plotting.parallel_coordinates(outliersRobust, 'cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c696408",
   "metadata": {},
   "source": [
    "### PCA Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd031ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PCAK=pd.concat([pcadf.reset_index(drop=True), pd.DataFrame(scores_pca)], axis = 1)\n",
    "df_PCAK.columns.values[-3: ] = ['PC 1', 'PC 2', 'PC 3']\n",
    "df_PCAK[\"K-Means PCA\"] = kmeans_pca.labels_\n",
    "\n",
    "df_PCAK.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695af886",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PCAK['Segment'] = df_PCAK['K-Means PCA'].map({0: 'first',\n",
    "                                               1:'second',\n",
    "                                               2: 'third',\n",
    "                                                 3: 'fourth',\n",
    "                                                 4: 'fifth',\n",
    "                                                 5: 'sixth',\n",
    "                                                 6: 'seventh',\n",
    "                                                 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8702037e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_axis = df_PCAK['PC 1']\n",
    "y_axis = df_PCAK['PC 2']\n",
    "plt.figure(figsize = (10,8))\n",
    "sns.scatterplot(x_axis, y_axis, hue = df_PCAK['Segment'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c497ac65",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 4 #\n",
    "*Summarize the Ramifications*\n",
    "\n",
    "Unfortunately, our clustering models were based on what was essentially a snippet of a much larger dataset. This was done based on need because of the limitations of the hardware on our team's computers, which lacked the computational power to analyze the original dataset without suffering from extreme performance lag. The ramifications of our decision to significantly cut the dataset are that our models are unlikely to reflect the actual trends that exist in the dataset as a whole. As such, the models described herein cannot be generalized to the population as a whole, rather they are only reflective of the students within the subsample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80334583",
   "metadata": {},
   "source": [
    "# Deployment #\n",
    "*Be critical of your performance and tell the reader how you current model might be usable by other parties. Did you achieve your goals? If not, can you reign in the utility of your modeling? How useful is your model for interested parties (i.e., the companies or organizations that might want to use it)? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12720aaa",
   "metadata": {},
   "source": [
    "Taken as a whole, the OULAD is clearly an important dataset that can be used to better understand the factors that affect online students' assessment results and to better address those factors that can increase or decrease students' performance in an online environment. \n",
    "\n",
    "For this particular project, we were able to take a random subsample of the data and successfully create a series of clustering models with an accuracy of 80% by clustering \"Number of Previous Attempts\" and the \"Pass\" level from \"Final Result\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a9f190",
   "metadata": {},
   "source": [
    "# Exceptional Work #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a2a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emp & al stuff\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "y = ohdf['final_result_Pass']\n",
    "X = ohdf[['num_of_prev_attempts_0','num_of_prev_attempts_1','num_of_prev_attempts_2','score', 'age_band_0-35']]\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "# run at n =100\n",
    "clf = RandomForestClassifier(n_estimators=2,random_state= randomState)\n",
    "\n",
    "acc = cross_val_score(clf,X,y=y,cv=cv)\n",
    "\n",
    "print (\"Average accuracy = \", acc.mean()*100, \"+-\", acc.std()*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
