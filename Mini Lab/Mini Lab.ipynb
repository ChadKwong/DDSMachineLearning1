{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bea4abb4",
   "metadata": {},
   "source": [
    "# Mini Lab: SVM and Logistic Regression Modeling\n",
    "\n",
    "### Teammates: Chad Kwong, Erin McClure-Price, Alex Lopez, Chris Haub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2faeb18",
   "metadata": {},
   "source": [
    "## Initial loading of dataset and all related packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073c62ef",
   "metadata": {},
   "source": [
    "The first step was to load necessary packages and the dataset. In addition, we created a series of variables for indexing both the continuous and categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c6d8211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import glob\n",
    "import os\n",
    "\n",
    "#Suppress futurewarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#Set Figure Size\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "#Import Dataset from github repo\n",
    "bankData = pd.read_csv('bank-additional-full.csv', sep=';', na_values=\"unknown\")\n",
    "#Optional: Add in index column\n",
    "#bankData.insert(0, 'Sample_ID', range(1,len(bankData)+1))\n",
    "\n",
    "#Creating variables for indexing continuous and categorical variables\n",
    "conCol = ['age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate', \n",
    "          'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "catCol = ['job', 'marital', 'education', 'default', 'housing', 'loan', \n",
    "          'contact', 'month', 'day_of_week', 'poutcome', 'y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e348214",
   "metadata": {},
   "source": [
    "The next step was to drop entries with missing values, this removed #### rows leaving 30,488 rows for analysis. In addition, we removed the 'default' column because it consisted of entirely 'no' responses and a total of 3 'yes' responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76bfe195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>59</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age        job  marital            education default housing loan  \\\n",
       "0   56  housemaid  married             basic.4y      no      no   no   \n",
       "2   37   services  married          high.school      no     yes   no   \n",
       "3   40     admin.  married             basic.6y      no      no   no   \n",
       "4   56   services  married          high.school      no      no  yes   \n",
       "6   59     admin.  married  professional.course      no      no   no   \n",
       "\n",
       "     contact month day_of_week  ...  campaign  pdays  previous     poutcome  \\\n",
       "0  telephone   may         mon  ...         1    999         0  nonexistent   \n",
       "2  telephone   may         mon  ...         1    999         0  nonexistent   \n",
       "3  telephone   may         mon  ...         1    999         0  nonexistent   \n",
       "4  telephone   may         mon  ...         1    999         0  nonexistent   \n",
       "6  telephone   may         mon  ...         1    999         0  nonexistent   \n",
       "\n",
       "  emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
       "0          1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "2          1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "3          1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "4          1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "6          1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankData2 = bankData.dropna()\n",
    "#remove 'default' column\n",
    "bankData2.drop(['default'], axis=1)\n",
    "bankData2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b388a0",
   "metadata": {},
   "source": [
    "We double checked the dataset to ensure that all missing values had been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e076bc7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "job               0\n",
       "marital           0\n",
       "education         0\n",
       "default           0\n",
       "housing           0\n",
       "loan              0\n",
       "contact           0\n",
       "month             0\n",
       "day_of_week       0\n",
       "duration          0\n",
       "campaign          0\n",
       "pdays             0\n",
       "previous          0\n",
       "poutcome          0\n",
       "emp.var.rate      0\n",
       "cons.price.idx    0\n",
       "cons.conf.idx     0\n",
       "euribor3m         0\n",
       "nr.employed       0\n",
       "y                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checked for missing values, no missing values\n",
    "bankData2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92218cd",
   "metadata": {},
   "source": [
    "Checking the shape of the data showed that we had a total of 30,488 rows and 21 attributes in our cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cb93304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30488, 21)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankData2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855ee1fc",
   "metadata": {},
   "source": [
    "Before we could build a model, we needed to create a dataframe with the target variable, y, which was whether a customer decided to take on a long term deposit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c19417e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    no\n",
       "2    no\n",
       "3    no\n",
       "4    no\n",
       "6    no\n",
       "Name: y, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define the target variable\n",
    "y = bankData2.y\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1653ad6",
   "metadata": {},
   "source": [
    "The dataset is a mixture of both continuous, numeric, and categorical data, therefore we chose to separate the categorical attributes into a separate dataframe and apply One Hot Encoding to convert categorical entries to numeric data. The converted dataframe was concatenated back into the original dataframe and the original attributes were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5979dc47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_blue-collar</th>\n",
       "      <th>job_entrepreneur</th>\n",
       "      <th>job_housemaid</th>\n",
       "      <th>job_management</th>\n",
       "      <th>job_retired</th>\n",
       "      <th>job_self-employed</th>\n",
       "      <th>job_services</th>\n",
       "      <th>job_student</th>\n",
       "      <th>job_technician</th>\n",
       "      <th>job_unemployed</th>\n",
       "      <th>...</th>\n",
       "      <th>month_dec</th>\n",
       "      <th>month_jul</th>\n",
       "      <th>month_jun</th>\n",
       "      <th>month_mar</th>\n",
       "      <th>month_may</th>\n",
       "      <th>month_nov</th>\n",
       "      <th>month_oct</th>\n",
       "      <th>month_sep</th>\n",
       "      <th>poutcome_nonexistent</th>\n",
       "      <th>poutcome_success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   job_blue-collar  job_entrepreneur  job_housemaid  job_management  \\\n",
       "0                0                 0              1               0   \n",
       "2                0                 0              0               0   \n",
       "3                0                 0              0               0   \n",
       "4                0                 0              0               0   \n",
       "6                0                 0              0               0   \n",
       "\n",
       "   job_retired  job_self-employed  job_services  job_student  job_technician  \\\n",
       "0            0                  0             0            0               0   \n",
       "2            0                  0             1            0               0   \n",
       "3            0                  0             0            0               0   \n",
       "4            0                  0             1            0               0   \n",
       "6            0                  0             0            0               0   \n",
       "\n",
       "   job_unemployed  ...  month_dec  month_jul  month_jun  month_mar  month_may  \\\n",
       "0               0  ...          0          0          0          0          1   \n",
       "2               0  ...          0          0          0          0          1   \n",
       "3               0  ...          0          0          0          0          1   \n",
       "4               0  ...          0          0          0          0          1   \n",
       "6               0  ...          0          0          0          0          1   \n",
       "\n",
       "   month_nov  month_oct  month_sep  poutcome_nonexistent  poutcome_success  \n",
       "0          0          0          0                     1                 0  \n",
       "2          0          0          0                     1                 0  \n",
       "3          0          0          0                     1                 0  \n",
       "4          0          0          0                     1                 0  \n",
       "6          0          0          0                     1                 0  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encoding of ALL categorical variables\n",
    "\n",
    "# pd.concat([*]], axis=1) // this line of code concatenates all the data frames in the [*] list\n",
    "# [** for col in categ_features] // this steps through each feature in categ_features and \n",
    "#                                //   creates a new element in a list based on the output of **\n",
    "# pd.get_dummies(df_imputed[col],prefix=col) // this creates a one hot encoded dataframe of the variable=col (like code above)\n",
    "\n",
    "categ_features = ['job','marital','education','default','housing','loan','contact','month','poutcome'];\n",
    "\n",
    "OneHotDF = pd.concat([pd.get_dummies(bankData2[col],prefix=col,drop_first=True) for col in categ_features], axis=1)\n",
    "\n",
    "OneHotDF.head()\n",
    "\n",
    "#https://github.com/jakemdrew/DataMiningNotebooks/blob/master/01.%20Pandas.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f30a9d",
   "metadata": {},
   "source": [
    "# Creating and Adjusting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68decef5",
   "metadata": {},
   "source": [
    "Create a logistic regression model and a support vector machine model for the\n",
    "classification task involved with your dataset. Assess how well each model performs (use\n",
    "80/20 training/testing split for your data). Adjust parameters of the models to make them more\n",
    "accurate. If your dataset size requires the use of stochastic gradient descent, then linear kernel\n",
    "only is fine to use. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307c8c7f",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60f8f40",
   "metadata": {},
   "source": [
    "Before building the models, we first separated out the Features and the Target for establishing testing and training sets. Once the features were separated, we performed cross validation with 2 splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb472615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating out the features for training and testing\n",
    "X = pd.concat([bankData2.select_dtypes(exclude='object'),OneHotDF],axis=1)\n",
    "X = X.values\n",
    "\n",
    "# Separating out target for training and testing\n",
    "Y = bankData2.pop(\"y\")\n",
    "Y = Y.values.ravel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29a0e6a",
   "metadata": {},
   "source": [
    "We then performed Cross Validation with 3 splits using a basic Logistic Regression model to compare accuracies between iterations of cross validation performed on the test and training splits. We also set the random state to 1 for replicability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9d28d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 1  ====\n",
      "accuracy: 0.9022630370613316\n",
      "confusion matrix\n",
      " [[5172  164]\n",
      " [ 432  330]]\n",
      "====Iteration 2  ====\n",
      "accuracy: 0.9050508363397836\n",
      "confusion matrix\n",
      " [[5188  157]\n",
      " [ 422  331]]\n",
      "====Iteration 3  ====\n",
      "accuracy: 0.8934076746474254\n",
      "confusion matrix\n",
      " [[5099  170]\n",
      " [ 480  349]]\n"
     ]
    }
   ],
   "source": [
    "#Cross validation, using 80/20 train/test splitting\n",
    "splits = 3\n",
    "randomState=1\n",
    "cv = ShuffleSplit(n_splits=splits, test_size=0.20, random_state=randomState)\n",
    "\n",
    "#Creating standard scaler object\n",
    "scl_obj = StandardScaler()\n",
    "\n",
    "#Creating Logistic Regression object\n",
    "lr_clf = LogisticRegression()\n",
    "\n",
    "#Setting Iteration start point for for loop\n",
    "iter_num=1\n",
    "\n",
    "#For loop for creating testing and training sets and running a full basic LR model to compare iterations the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv.split(X,Y): \n",
    "    X_train = X[train_indices]\n",
    "    Y_train = Y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    Y_test = Y[test_indices] \n",
    "    \n",
    "    X_train_scaled = scl_obj.fit_transform(X_train) # apply to training\n",
    "    X_test_scaled = scl_obj.transform(X_test) \n",
    "    \n",
    "    lr_clf.fit(X_train_scaled, Y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "    acc = mt.accuracy_score(Y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(Y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print('accuracy:', acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    #print(conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time\n",
    "    \n",
    "\n",
    "# https://github.com/jakemdrew/DataMiningNotebooks/blob/master/04.%20Logits%20and%20SVM.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39cec4",
   "metadata": {},
   "source": [
    "The following cel of code uses the ipywidgets package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c5b2bebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Parameter Tuning:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902d3612ea55432993fba46eaf30dd24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='penMethod', options=('l2', 'l1', 'elasticnet', 'none'), value='l2'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.exploreLogisticRegression(penMethod, cost, algorithm, maxIterations, verboseN, dualOption, tolerance)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we can change some of the parameters interactively\n",
    "from ipywidgets import widgets as wd\n",
    "\n",
    "def exploreLogisticRegression(penMethod, cost, algorithm, maxIterations, verboseN, dualOption, tolerance):\n",
    "    model = LogisticRegression(random_state=randomState,\n",
    "                               penalty='l2',\n",
    "                               C=cost,\n",
    "                               class_weight=None,\n",
    "                               solver=algorithm,\n",
    "                               max_iter = maxIterations,\n",
    "                               verbose = verboseN,\n",
    "                               dual = dualOption,\n",
    "                               tol = tolerance * 1e-4)\n",
    "    acc = cross_val_score(model,X,y=y,cv=cv)\n",
    "    print(\"\\n\\nUsing the settings above:\\n\\nThe first iteration accuracy is\",acc[0])\n",
    "    print(\"The second iteration accuracy is\",acc[1])\n",
    "    print(\"The third iteration accuracy is\",acc[2])\n",
    "\n",
    "print('Logistic Regression Parameter Tuning:')\n",
    "#Default values:\n",
    "#PenMethod = l2, cost = .05, algorithm = lbfgs, maxIterations = 100\n",
    "#verboseN = 0, dualOption = False, tolerance = 1e-4 or 1 on the sliders,\n",
    "#\n",
    "wd.interact(exploreLogisticRegression,\n",
    "            penMethod=['l2','l1','elasticnet', 'none'],\n",
    "            cost=(0.001,.5,0.05),\n",
    "            algorithm = ['liblinear', 'saga', 'sag', 'newton-cg', 'lbfgs'],\n",
    "            maxIterations = (50,500,10),\n",
    "            verboseN = (0,10,1),\n",
    "            dualOption = [False, True],\n",
    "            tolerance = (1,10,1),\n",
    "            __manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0764a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8925877336831748\n",
      "Confusion Matrix:\n",
      " [[5103  166]\n",
      " [ 489  340]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegression(penalty='l2', C=0.05, solver='liblinear')\n",
    "lr_clf.fit(X_train_scaled,Y_train)\n",
    "y_hat = lr_clf.predict(X_test_scaled)\n",
    "acc = mt.accuracy_score(Y_test,y_hat)\n",
    "conf = mt.confusion_matrix(Y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(\"Confusion Matrix:\\n\",conf,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac368c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8902918989832732\n",
      "Confusion Matrix:\n",
      " [[5099  170]\n",
      " [ 499  330]] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chadkwong/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegression(penalty='l2', C=0.05, solver='saga')\n",
    "lr_clf.fit(X_train_scaled,Y_train)\n",
    "y_hat = lr_clf.predict(X_test_scaled)\n",
    "acc = mt.accuracy_score(Y_test,y_hat)\n",
    "conf = mt.confusion_matrix(Y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(\"Confusion Matrix:\\n\",conf,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1f424b",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a604710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model\n",
    "svm_clf = SVC(C=0.5, kernel='rbf', degree=3, gamma='auto')\n",
    "svm_clf.fit(X_train_scaled, Y_train)\n",
    "\n",
    "#Generating Predictions\n",
    "y_hat = svm_clf.predict(X_test_scaled)\n",
    "\n",
    "#calculating accuracy and confusion matrix and printing the two\n",
    "acc = mt.accuracy_score(Y_test,y_hat)\n",
    "conf = mt.confusion_matrix(Y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(\"Confusion Matrix:\\n\",conf,\"\\n\")\n",
    "\n",
    "#Printing Support Vector shapes and information\n",
    "print(\"The shape of the support vectors is \",svm_clf.support_vectors_.shape)\n",
    "print(\"The indices of support vectors are\",svm_clf.support_.shape)\n",
    "print(\"The numbers of support vectors for each class are\",svm_clf.n_support_, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f804f08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make a dataframe of the training data\n",
    "df_tested_on = bankData2.iloc[train_indices].copy() # saved from above, the indices chosen for training\n",
    "# getting the support vectors from the trained model\n",
    "df_support = bankData2.iloc[svm_clf.support_,:].copy()\n",
    "\n",
    "#Adding y column back into datasets\n",
    "df_support[\"y\"] = Y[svm_clf.support_] \n",
    "bankData2[\"y\"] = Y # also add it back in for the original data\n",
    "\n",
    "# group the original data and the support vectors\n",
    "df_grouped_support = df_support.groupby([\"y\"])\n",
    "df_grouped = bankData2.groupby([\"y\"])\n",
    "\n",
    "#plotting KDE of all continuous variables\n",
    "for v in bankData[conCol].columns:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    # plot support vector stats\n",
    "    plt.subplot(1,2,1)\n",
    "    ax = df_grouped_support[v].plot.kde() \n",
    "    plt.legend([\"n\",\"y\"])\n",
    "    plt.title(v+' (Instances chosen as Support Vectors)')\n",
    "    \n",
    "    # plot original distributions\n",
    "    plt.subplot(1,2,2)\n",
    "    ax = df_grouped[v].plot.kde() \n",
    "    plt.legend(['y','n'])\n",
    "    plt.title(v+' (Original)')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6870d662",
   "metadata": {},
   "source": [
    "You can also look at joint plots of the data and see how relationships have changed. \n",
    "**(Hint hint for the min-lab assignment--this would be a nice analysis of the support vectors.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4942c91a",
   "metadata": {},
   "source": [
    "# Discussing Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f135175",
   "metadata": {},
   "source": [
    "Discuss the advantages of each model for each classification task. Does one type\n",
    "of model offer superior performance over another in terms of prediction accuracy? In terms of\n",
    "training time or efficiency? Explain in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f909e23",
   "metadata": {},
   "source": [
    "# Interpretting weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e422d",
   "metadata": {},
   "source": [
    "Use the weights from logistic regression to interpret the importance of different\n",
    "features for each classification task. Explain your interpretation in detail. Why do you think\n",
    "some variables are more important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbf2045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing weights on a normalized scale\n",
    "print('The following is a breakdown of the coefficient weights:\\n')\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(abs(lr_clf.coef_.T),pd.concat([bankData2.select_dtypes(exclude='object'),OneHotDF],axis=1).columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars, reverse=True)\n",
    "\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out\n",
    "#Plot magnitude of weights in descending order\n",
    "#Grabbing column names for labels\n",
    "#columns = pd.concat([bankData2.select_dtypes(exclude='object'),OneHotDF],axis=0).columns\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#Set Figure Size\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "#Plotting weights\n",
    "labels = pd.concat([bankData2.select_dtypes(exclude='object'),OneHotDF],axis=1).columns\n",
    "weights = abs(pd.Series(lr_clf.coef_[0],index=labels)).sort_values(ascending=False)\n",
    "weights.plot(kind='bar', title=\"Absolute Value of Weight Coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e631b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(penalty='l2', C=0.05, solver='liblinear', max_iter=100)\n",
    "lr_clf.fit(X_train_scaled,Y_train)\n",
    "y_hat = lr_clf.predict(X_test_scaled)\n",
    "print('The following is a breakdown of the coefficient weights:\\n')\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(abs(lr_clf.coef_.T),pd.concat([bankData2.select_dtypes(exclude='object'),OneHotDF],axis=1).columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars, reverse=True)\n",
    "\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out\n",
    "#Plot magnitude of weights in descending order\n",
    "#Grabbing column names for labels\n",
    "#columns = pd.concat([bankData2.select_dtypes(exclude='object'),OneHotDF],axis=0).columns\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#Set Figure Size\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "#Plotting weights\n",
    "labels = pd.concat([bankData2.select_dtypes(exclude='object'),OneHotDF],axis=1).columns\n",
    "weights = abs(pd.Series(lr_clf.coef_[0],index=labels)).sort_values(ascending=False)\n",
    "weights.plot(kind='bar', title=\"Absolute Value of Weight Coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9550e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(penalty='l2', C=0.05, solver='saga', max_iter=100)\n",
    "lr_clf.fit(X_train_scaled,Y_train)\n",
    "y_hat = lr_clf.predict(X_test_scaled)\n",
    "acc = mt.accuracy_score(Y_test,y_hat)\n",
    "print(acc)\n",
    "labels = pd.concat([bankData2.select_dtypes(exclude='object'),OneHotDF],axis=1).columns\n",
    "weights = abs(pd.Series(lr_clf.coef_[0],index=labels)).sort_values(ascending=False)\n",
    "weights.plot(kind='bar', title=\"Absolute Value of Weight Coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad92e2c",
   "metadata": {},
   "source": [
    "## Gradient Based Alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8388c56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load another dataset (large) and train using various methods of gradient (and mini-batch)\n",
    "# from __future__ import print_function\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96404fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#https://github.com/jakemdrew/DataMiningNotebooks/blob/master/04.%20Logits%20and%20SVM.ipynb\n",
    "# use some compact notation for creating a linear SVM classifier with stichastic descent\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "regularize_const = 0.1\n",
    "iterations = 5\n",
    "svm_sgd = SGDClassifier(alpha=regularize_const,\n",
    "        fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
    "        loss='hinge', n_iter_no_change=iterations, n_jobs=-1, penalty='l2')\n",
    "\n",
    "scl = StandardScaler()\n",
    "for train_idx, test_idx in cv.split(X,Y):\n",
    "    svm_sgd.fit(scl.fit_transform(X[train_idx]),Y[train_idx])\n",
    "    #Generating predictions\n",
    "    yhat = svm_sgd.predict(scl.transform(X[test_idx]))\n",
    "    #Generating Confusion Matrix and Accuracy Score\n",
    "    conf = mt.confusion_matrix(Y[test_idx],yhat)\n",
    "    acc = mt.accuracy_score(Y[test_idx],yhat)\n",
    "\n",
    "print('SVM:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3944939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# use some compact notation for creating a logistic regression classifier with stochastic descent\n",
    "log_sgd = SGDClassifier(alpha=regularize_const,\n",
    "        fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
    "        loss='log', n_iter_no_change=iterations, n_jobs=-1, penalty='l2')\n",
    "\n",
    "scl = StandardScaler()\n",
    "for train_idx, test_idx in cv.split(X,Y):\n",
    "    log_sgd.fit(scl.fit_transform(X[train_idx]),Y[train_idx])\n",
    "    yhatLR = log_sgd.predict(scl.transform(X[test_idx]))\n",
    "    \n",
    "    conf = mt.confusion_matrix(Y[test_idx],yhatLR)\n",
    "    acc = mt.accuracy_score(Y[test_idx],yhatLR)\n",
    "\n",
    "print('Logistic Regression:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71dd5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# now lets use some of what we know from this class to reduce the dimensionality of the set\n",
    "from sklearn.decomposition import PCA\n",
    "n_components = 43\n",
    "\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized')\n",
    "\n",
    "iterations = 150\n",
    "log_sgd = SGDClassifier(\n",
    "        fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
    "        loss='log', max_iter=iterations, n_jobs=-1, penalty='l2')\n",
    "\n",
    "# you could also set this up in a pipeline\n",
    "for train_idx, test_idx in cv.split(X,Y):\n",
    "    log_sgd.fit(pca.fit_transform(X[train_idx]),Y[train_idx])\n",
    "    yhat = log_sgd.predict(pca.transform(X[test_idx]))\n",
    "    \n",
    "    conf = mt.confusion_matrix(Y[test_idx],yhat)\n",
    "    acc = mt.accuracy_score(Y[test_idx],yhat)\n",
    "\n",
    "print('Logistic Regression:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdf0a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(8,8))\n",
    "plt.imshow(conf)\n",
    "\n",
    "#Confusion Matrix\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152f2cd6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41798fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "yBinary = pd.Series(np.where(Y == 'yes', 1, 0),y.index)\n",
    "yhatBinary = pd.Series(np.where(yhat == 'yes', 1, 0))\n",
    "yhatLRBinary = pd.Series(np.where(yhatLR == 'yes', 1, 0))\n",
    "logLossSVM = log_loss(Y[test_idx],yhatBinary)\n",
    "logLossLR = log_loss(Y[test_idx],yhatLRBinary)\n",
    "\n",
    "print('The Log Loss of the logistic regression model is: ', logLossLR)\n",
    "print('The Log Loss of the SVM model is: ', logLossSVM)\n",
    "print((logLossLR - logLossSVM)/logLossLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7e258e",
   "metadata": {},
   "source": [
    "When it comes to log-loss scores, the model with the lower score is superior. With this logic, we can see that the SVM model performs better by about 7% (3.5287 < 3.6023) when it comes to full models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf622180",
   "metadata": {},
   "source": [
    "# SVM Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba3edab",
   "metadata": {},
   "source": [
    "Look at the chosen support vectors for the classification task. Do these provide\n",
    "any insight into the data? Explain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
