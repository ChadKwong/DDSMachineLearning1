{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4966374",
   "metadata": {},
   "source": [
    "# Mini Lab: SVM and Logistic Regression Modeling\n",
    "\n",
    "### Teammates: Chad Kwong, Erin McClure-Price, Alex Lopez, Chris Haub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e461057",
   "metadata": {},
   "source": [
    "## Initial loading of dataset and all related packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a4d238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "\n",
    "#Suppress futurewarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#Set Figure Size\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "\n",
    "\n",
    "#Import Dataset from github repo\n",
    "bankData = pd.read_csv('bank-additional-full.csv', sep=';', na_values=\"unknown\")\n",
    "#Optional: Add in index column\n",
    "#bankData.insert(0, 'Sample_ID', range(1,len(bankData)+1))\n",
    "\n",
    "#Creating variables for indexing continuous and categorical variables\n",
    "conCol = ['age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate', \n",
    "          'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "catCol = ['job', 'marital', 'education', 'default', 'housing', 'loan', \n",
    "          'contact', 'month', 'day_of_week', 'poutcome', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd435f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>59</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age        job  marital            education default housing loan  \\\n",
       "0   56  housemaid  married             basic.4y      no      no   no   \n",
       "2   37   services  married          high.school      no     yes   no   \n",
       "3   40     admin.  married             basic.6y      no      no   no   \n",
       "4   56   services  married          high.school      no      no  yes   \n",
       "6   59     admin.  married  professional.course      no      no   no   \n",
       "\n",
       "     contact month day_of_week  ...  campaign  pdays  previous     poutcome  \\\n",
       "0  telephone   may         mon  ...         1    999         0  nonexistent   \n",
       "2  telephone   may         mon  ...         1    999         0  nonexistent   \n",
       "3  telephone   may         mon  ...         1    999         0  nonexistent   \n",
       "4  telephone   may         mon  ...         1    999         0  nonexistent   \n",
       "6  telephone   may         mon  ...         1    999         0  nonexistent   \n",
       "\n",
       "  emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
       "0          1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "2          1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "3          1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "4          1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "6          1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankData2 = bankData.dropna()\n",
    "#remove 'default' column\n",
    "bankData2.drop(['default'], axis=1)\n",
    "bankData2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7983f9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "job               0\n",
       "marital           0\n",
       "education         0\n",
       "default           0\n",
       "housing           0\n",
       "loan              0\n",
       "contact           0\n",
       "month             0\n",
       "day_of_week       0\n",
       "duration          0\n",
       "campaign          0\n",
       "pdays             0\n",
       "previous          0\n",
       "poutcome          0\n",
       "emp.var.rate      0\n",
       "cons.price.idx    0\n",
       "cons.conf.idx     0\n",
       "euribor3m         0\n",
       "nr.employed       0\n",
       "y                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checked for missing values, no missing values\n",
    "bankData2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85b4b230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30488, 21)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankData2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e7012",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4a6840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = linear_model.Regression()\n",
    "#model.fit(bankData, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a64d37b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    no\n",
       "2    no\n",
       "3    no\n",
       "4    no\n",
       "6    no\n",
       "Name: y, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define the target variable\n",
    "y = bankData2.y\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96807cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_blue-collar</th>\n",
       "      <th>job_entrepreneur</th>\n",
       "      <th>job_housemaid</th>\n",
       "      <th>job_management</th>\n",
       "      <th>job_retired</th>\n",
       "      <th>job_self-employed</th>\n",
       "      <th>job_services</th>\n",
       "      <th>job_student</th>\n",
       "      <th>job_technician</th>\n",
       "      <th>job_unemployed</th>\n",
       "      <th>...</th>\n",
       "      <th>month_dec</th>\n",
       "      <th>month_jul</th>\n",
       "      <th>month_jun</th>\n",
       "      <th>month_mar</th>\n",
       "      <th>month_may</th>\n",
       "      <th>month_nov</th>\n",
       "      <th>month_oct</th>\n",
       "      <th>month_sep</th>\n",
       "      <th>poutcome_nonexistent</th>\n",
       "      <th>poutcome_success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   job_blue-collar  job_entrepreneur  job_housemaid  job_management  \\\n",
       "0                0                 0              1               0   \n",
       "2                0                 0              0               0   \n",
       "3                0                 0              0               0   \n",
       "4                0                 0              0               0   \n",
       "6                0                 0              0               0   \n",
       "\n",
       "   job_retired  job_self-employed  job_services  job_student  job_technician  \\\n",
       "0            0                  0             0            0               0   \n",
       "2            0                  0             1            0               0   \n",
       "3            0                  0             0            0               0   \n",
       "4            0                  0             1            0               0   \n",
       "6            0                  0             0            0               0   \n",
       "\n",
       "   job_unemployed  ...  month_dec  month_jul  month_jun  month_mar  month_may  \\\n",
       "0               0  ...          0          0          0          0          1   \n",
       "2               0  ...          0          0          0          0          1   \n",
       "3               0  ...          0          0          0          0          1   \n",
       "4               0  ...          0          0          0          0          1   \n",
       "6               0  ...          0          0          0          0          1   \n",
       "\n",
       "   month_nov  month_oct  month_sep  poutcome_nonexistent  poutcome_success  \n",
       "0          0          0          0                     1                 0  \n",
       "2          0          0          0                     1                 0  \n",
       "3          0          0          0                     1                 0  \n",
       "4          0          0          0                     1                 0  \n",
       "6          0          0          0                     1                 0  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encoding of ALL categorical variables\n",
    "\n",
    "# pd.concat([*]], axis=1) // this line of code concatenates all the data frames in the [*] list\n",
    "# [** for col in categ_features] // this steps through each feature in categ_features and \n",
    "#                                //   creates a new element in a list based on the output of **\n",
    "# pd.get_dummies(df_imputed[col],prefix=col) // this creates a one hot encoded dataframe of the variable=col (like code above)\n",
    "\n",
    "categ_features = ['job','marital','education','default','housing','loan','contact','month','poutcome'];\n",
    "\n",
    "OneHotDF = pd.concat([pd.get_dummies(bankData2[col],prefix=col,drop_first=True) for col in categ_features], axis=1)\n",
    "\n",
    "OneHotDF.head()\n",
    "\n",
    "#https://github.com/jakemdrew/DataMiningNotebooks/blob/master/01.%20Pandas.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0f87031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['job_blue-collar', 'job_entrepreneur', 'job_housemaid',\n",
       "       'job_management', 'job_retired', 'job_self-employed', 'job_services',\n",
       "       'job_student', 'job_technician', 'job_unemployed', 'marital_married',\n",
       "       'marital_single', 'education_basic.6y', 'education_basic.9y',\n",
       "       'education_high.school', 'education_illiterate',\n",
       "       'education_professional.course', 'education_university.degree',\n",
       "       'default_yes', 'housing_yes', 'loan_yes', 'contact_telephone',\n",
       "       'month_aug', 'month_dec', 'month_jul', 'month_jun', 'month_mar',\n",
       "       'month_may', 'month_nov', 'month_oct', 'month_sep',\n",
       "       'poutcome_nonexistent', 'poutcome_success'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OneHotDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3abe3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas.pop\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pop.html\n",
    "Y = bankData2.pop(\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f748e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged OneHotDF and bankData files\n",
    "\n",
    "X = pd.concat([bankData2.select_dtypes(exclude='object'),OneHotDF],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6c25866",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating input data into two parts X (features) and Y (target)\n",
    "# features = ['age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate', \n",
    "#           'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed', 'job', 'marital', 'education', 'default', 'housing', 'loan', \n",
    "#           'contact', 'month', 'day_of_week', 'poutcome']\n",
    "# # Separating out the features\n",
    "# X = mergedDF.loc[:, features].values\n",
    "\n",
    "# #Separating out the target\n",
    "# Y = mergedDF.loc[:, ['y']].values.ravel()\n",
    "# Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c876c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30488, 43)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08bf711",
   "metadata": {},
   "source": [
    "## Training and Testing Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a246d87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Building a logistic regression model with default values for parameters\n",
    "\n",
    "#apply cross validation, using 80/20 train/test splitting\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "cv = ShuffleSplit(n_splits=2, test_size=0.20, random_state=0)\n",
    "\n",
    "#apply standard scaling: to yield standard normally distributed data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scl_obj = StandardScaler()\n",
    "\n",
    "#apply logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "regEstimator = LogisticRegression()\n",
    "\n",
    "X_train_scaled = scl_obj.fit_transform(X) # apply to training\n",
    "    \n",
    "    \n",
    "\n",
    "regEstimator.fit(X_train_scaled, Y)  # train object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "249774f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30488,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separating out the features\n",
    "X = X.values\n",
    "X.shape\n",
    "#Separating out the target\n",
    "Y = Y.values.ravel()\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70dbc66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e056eac",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2041074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy: 0.9017710724827812\n",
      "confusion matrix\n",
      " [[5172  178]\n",
      " [ 421  327]]\n",
      "====Iteration 1  ====\n",
      "accuracy: 0.9034109544112824\n",
      "confusion matrix\n",
      " [[5166  160]\n",
      " [ 429  343]]\n"
     ]
    }
   ],
   "source": [
    "#Building a logistic regression model with default values for parameters\n",
    "\n",
    "#apply cross validation, using 80/20 train/test splitting\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "cv = ShuffleSplit(n_splits=2, test_size=0.20, random_state=0)\n",
    "\n",
    "#apply standard scaling: to yield standard normally distributed data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scl_obj = StandardScaler()\n",
    "\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer, mean_squared_error\n",
    "\n",
    "#apply logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "regEstimator = LogisticRegression()\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv.split(X,Y): \n",
    "    X_train = X[train_indices]\n",
    "    Y_train = Y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    Y_test = Y[test_indices] \n",
    "    \n",
    "    X_train_scaled = scl_obj.fit_transform(X_train) # apply to training\n",
    "    X_test_scaled = scl_obj.transform(X_test) \n",
    "    \n",
    "    regEstimator.fit(X_train_scaled, Y_train)  # train object\n",
    "    y_hat = regEstimator.predict(X_test_scaled) # get test set precitions\n",
    "    acc = mt.accuracy_score(Y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(Y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print('accuracy:', acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    #print(conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time\n",
    "    \n",
    "\n",
    "# https://github.com/jakemdrew/DataMiningNotebooks/blob/master/04.%20Logits%20and%20SVM.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf243b0",
   "metadata": {},
   "source": [
    "## Interpretting weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "465909a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age has weight of 0.002986804448735319\n",
      "job has weight of 0.004523902603884739\n",
      "marital has weight of -0.04925514675752567\n",
      "education has weight of -0.001649833115260028\n",
      "default has weight of -0.15644221289832771\n",
      "housing has weight of -0.26242216004729624\n",
      "loan has weight of 0.34583433465796615\n",
      "contact has weight of 0.023767501225638386\n",
      "month has weight of -0.21470807520634727\n",
      "day_of_week has weight of -0.00639968736441983\n",
      "duration has weight of -0.12658270010675196\n",
      "campaign has weight of -0.019087778581731225\n",
      "pdays has weight of 0.002539695529330375\n",
      "previous has weight of 0.013906243310940472\n",
      "poutcome has weight of 0.06406019957357074\n",
      "emp.var.rate has weight of -0.0010645213505478892\n",
      "cons.price.idx has weight of -0.043345300840673034\n",
      "cons.conf.idx has weight of 0.03979010475719272\n",
      "euribor3m has weight of 0.018951309176585236\n",
      "nr.employed has weight of 0.004723955149351456\n"
     ]
    }
   ],
   "source": [
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None, solver='liblinear' ) # get object\n",
    "\n",
    "\n",
    "# train the reusable logisitc regression model on the training data\n",
    "lr_clf.fit(X_train,Y_train)  # train object\n",
    "y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "# interpret the weights\n",
    "\n",
    "# iterate over the coefficients\n",
    "weights = lr_clf.coef_.T # take transpose to make a column vector\n",
    "variable_names = bankData2.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])\n",
    "    \n",
    "# does this look correct? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4ea8926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24390, 43)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52ac758",
   "metadata": {},
   "source": [
    "These weight interpretations are not neccessarily interpretable because of the values we had. Very large attribute values could just as easily be assigned a higher weight. Instead, let's normalize the feature values so that all the attributes are on the same dynamic range. Once we normalize the attributes, the weights should have magnitudes that reflect their poredictive power in the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fb31ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9025910134470319\n",
      "[[5166  160]\n",
      " [ 434  338]]\n",
      "housing has weight of -1.2101307850356864\n",
      "day_of_week has weight of -0.3098294751394472\n",
      "education has weight of -0.19779140651253854\n",
      "marital has weight of -0.09421064320282992\n",
      "duration has weight of -0.0794879442653976\n",
      "cons.price.idx has weight of -0.039411962238107195\n",
      "default has weight of -0.03887983722575623\n",
      "campaign has weight of -0.02631792149995555\n",
      "age has weight of -0.02476762861571548\n",
      "emp.var.rate has weight of -0.011267302261191757\n",
      "nr.employed has weight of -0.001050750041491694\n",
      "euribor3m has weight of 0.002811198075874412\n",
      "previous has weight of 0.014187587999689484\n",
      "pdays has weight of 0.018157606483939305\n",
      "cons.conf.idx has weight of 0.029261916744363917\n",
      "contact has weight of 0.058322977583420095\n",
      "poutcome has weight of 0.0831858360380574\n",
      "month has weight of 0.14900016787710227\n",
      "loan has weight of 0.48261121208776697\n",
      "job has weight of 1.1548816933506276\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "# to Make things easier, let's start by just using whatever was last stored in the variables:\n",
    "##    X_train , y_train , X_test, y_test (they were set in a for loop above)\n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05, solver='liblinear') # get object, the 'C' value is less (can you guess why??)\n",
    "lr_clf.fit(X_train_scaled,Y_train)  # train object\n",
    "\n",
    "y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(Y_test,y_hat)\n",
    "conf = mt.confusion_matrix(Y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(lr_clf.coef_.T,bankData2.columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars)\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9c7e01",
   "metadata": {},
   "source": [
    "## Look at code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3cfc547a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mggplot\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m weights \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(lr_clf\u001b[38;5;241m.\u001b[39mcoef_[\u001b[38;5;241m0\u001b[39m],index\u001b[38;5;241m=\u001b[39m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m)\n\u001b[0;32m      8\u001b[0m weights\u001b[38;5;241m.\u001b[39mplot(kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "weights = pd.Series(lr_clf.coef_[0],index=X.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1601935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "print(len(lr_clf.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1175528",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02ae46cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, so run through the cross validation loop and set the training and testing variable for one single iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = Y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = Y[test_indices]\n",
    "    \n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test)\n",
    "\n",
    "# https://github.com/jakemdrew/DataMiningNotebooks/blob/master/04.%20Logits%20and%20SVM.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbb7c12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8935716628402754\n",
      "[[5173  107]\n",
      " [ 542  276]]\n"
     ]
    }
   ],
   "source": [
    "# lets investigate SVMs on the data and play with the parameters and kernels\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model just as before\n",
    "svm_clf = SVC(C=0.5, kernel='rbf', degree=3, gamma='auto') # get object\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03041fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5874, 43)\n",
      "(5874,)\n",
      "[3075 2799]\n"
     ]
    }
   ],
   "source": [
    "# look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce763ca9",
   "metadata": {},
   "source": [
    "## Need to add weights section below here for code below to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67ed9b7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "coef_ is only available when using a linear kernel",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# if using linear kernel, these make sense to look at (not otherwise, why?)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msvm_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef_\u001b[49m)\n\u001b[0;32m      3\u001b[0m weights \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(svm_clf\u001b[38;5;241m.\u001b[39mcoef_[\u001b[38;5;241m0\u001b[39m],index\u001b[38;5;241m=\u001b[39mbankData2\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m      4\u001b[0m weights\u001b[38;5;241m.\u001b[39mplot(kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\_base.py:637\u001b[0m, in \u001b[0;36mBaseLibSVM.coef_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;124;03m\"\"\"Weights assigned to the features when `kernel=\"linear\"`.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \n\u001b[0;32m    632\u001b[0m \u001b[38;5;124;03mReturns\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;124;03m-------\u001b[39;00m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;124;03mndarray of shape (n_features, n_classes)\u001b[39;00m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 637\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoef_ is only available when using a linear kernel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    639\u001b[0m coef \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_coef()\n\u001b[0;32m    641\u001b[0m \u001b[38;5;66;03m# coef_ being a read-only property, it's better to mark the value as\u001b[39;00m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;66;03m# immutable to avoid hiding potential bugs for the unsuspecting user.\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: coef_ is only available when using a linear kernel"
     ]
    }
   ],
   "source": [
    "# if using linear kernel, these make sense to look at (not otherwise, why?)\n",
    "print(svm_clf.coef_)\n",
    "weights = pd.Series(svm_clf.coef_[0],index=bankData2.columns)\n",
    "weights.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff7f0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using linear kernel, these make sense to look at (not otherwise, why?)\n",
    "# print(svm_clf.coef_)\n",
    "# weights = pd.Series(svm_clf.coef_[0],index=df_imputed.columns)\n",
    "# weights.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e201f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do some different analysis with the SVM and look at the instances that were chosen as support vectors\n",
    "\n",
    "# now lets look at the support for the vectors and see if we they are indicative of anything\n",
    "# grabe the rows that were selected as support vectors (these are usually instances that are hard to classify)\n",
    "\n",
    "# make a dataframe of the training data\n",
    "df_tested_on = bankData2.iloc[train_indices].copy() # saved from above, the indices chosen for training\n",
    "# now get the support vectors from the trained model\n",
    "df_support = bankData2.iloc[svm_clf.support_,:].copy()\n",
    "\n",
    "df_support[\"y\"] = Y[svm_clf.support_] # add back in the 'Yes' Column to the pandas dataframe\n",
    "bankData2[\"y\"] = Y # also add it back in for the original data\n",
    "df_support.info()\n",
    "\n",
    "# https://github.com/jakemdrew/DataMiningNotebooks/blob/master/04.%20Logits%20and%20SVM.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb789c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets see the statistics of these attributes\n",
    "from pandas.plotting import boxplot\n",
    "\n",
    "# group the original data and the support vectors\n",
    "df_grouped_support = df_support.groupby([\"y\"])\n",
    "df_grouped = bankData2.groupby([\"y\"])\n",
    "\n",
    "# plot KDE of Different variables\n",
    "vars_to_plot = ['age','campaign','previous','nr.employed']\n",
    "\n",
    "for v in vars_to_plot:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    # plot support vector stats\n",
    "    plt.subplot(1,2,1)\n",
    "    ax = df_grouped_support[v].plot.kde() \n",
    "    plt.legend([\"n\",\"y\"])\n",
    "    plt.title(v+' (Instances chosen as Support Vectors)')\n",
    "    \n",
    "    # plot original distributions\n",
    "    plt.subplot(1,2,2)\n",
    "    ax = df_grouped[v].plot.kde() \n",
    "    plt.legend(['y','n'])\n",
    "    plt.title(v+' (Original)')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ecc334",
   "metadata": {},
   "source": [
    "You can also look at joint plots of the data and see how relationships have changed. \n",
    "**(Hint hint for the min-lab assignment--this would be a nice analysis of the support vectors.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db457e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluating Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
